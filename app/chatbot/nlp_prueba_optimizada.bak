import time
import psutil
import os
import sys
import subprocess
import spacy
import json
import logging
from typing import Dict, List, Union
from spacy.matcher import PhraseMatcher
from cachetools import cachedmethod, TTLCache
import ijson
from datetime import datetime
import logging.handlers

# Configurar logging
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
log_file = f"logs/optimizado_nuevo_{timestamp}.log"
os.makedirs("logs", exist_ok=True)
handler = logging.handlers.RotatingFileHandler(log_file, maxBytes=10*1024*1024, backupCount=5)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[handler, logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# Rutas a los catálogos
CATALOG_PATHS = {
    "global": {"esco": "/home/pablollh/skills_data/ESCO_occup_skills.json"},
    "es": {
        "skills_relax": "/home/pablollh/skills_data/skill_db_relax_20_pretty.json",
        "skills_opportunities": "/home/pablollh/app/utilidades/catalogs/skills.json"
    }
}
COMBINED_SKILLS_FILE = "/home/pablollh/skills_data/skills_combined.json"

# Lista de textos de prueba (reducida)
texts = [
    "Tengo amplia experiencia en Python, Django y liderazgo en equipos.",
    "I have strong skills in JavaScript, React, and agile methodologies.",
    "Soy CFA y tengo conocimientos avanzados en análisis financiero.",
    "Conozco técnicas de marketing digital, SEO y gestión de redes sociales.",
    "Experto en robótica, automatización industrial y programación en C++."
]

class ModelManager:
    def __init__(self):
        self.model_cache = TTLCache(maxsize=10, ttl=21600)

    def get_model(self, lang: str) -> spacy.language.Language:
        cache_key = f"{lang}_md"
        if cache_key in self.model_cache:
            return self.model_cache[cache_key]
        model_name = {
            "es": "es_core_news_md",
            "default": "xx_ent_wiki_sm"
        }.get(lang, "xx_ent_wiki_sm")
        try:
            model = spacy.load(model_name)
        except OSError:
            logger.info(f"Descargando modelo {model_name}...")
            subprocess.check_call([sys.executable, "-m", "spacy", "download", model_name])
            model = spacy.load(model_name)
        self.model_cache[cache_key] = model
        return model

model_manager = ModelManager()

# Funciones optimizadas para cargar catálogos
def stream_dict_json(file_path: str, key_field: str = "preferredLabel.es") -> Dict[str, str]:
    result = {}
    try:
        with open(file_path, 'rb') as f:
            parser = ijson.parse(f)
            current_key = None
            for prefix, event, value in parser:
                if event == "map_key" and not prefix:
                    current_key = value
                elif prefix.endswith(key_field) and event == "string":
                    result[current_key] = value.lower()
        logger.info(f"Cargado {file_path} con {len(result)} entradas.")
        return result
    except Exception as e:
        logger.error(f"Error cargando {file_path}: {e}")
        return {}

def stream_skill_db(file_path: str) -> set[str]:
    skills = set()
    try:
        with open(file_path, 'rb') as f:
            parser = ijson.parse(f)
            for prefix, event, value in parser:
                if prefix.endswith("skill_name") and event == "string":
                    skills.add(value.lower())
                elif prefix.endswith("full") and event == "string":
                    skills.add(value.lower())
        logger.info(f"Cargado {file_path} con {len(skills)} habilidades - Ejemplos: {list(skills)[:10]}")
        return skills
    except Exception as e:
        logger.error(f"Error cargando {file_path}: {e}")
        return set()

def load_skills_opportunities(file_path: str) -> set[str]:
    skills = set()
    try:
        with open(file_path, 'rb') as f:
            parser = ijson.parse(f)
            for prefix, event, value in parser:
                if prefix.endswith("Habilidades Técnicas.item") or prefix.endswith("Habilidades Blandas.item"):
                    if event == "string":
                        skills.add(value.lower())
        logger.info(f"Cargado {file_path} con {len(skills)} elementos.")
        return skills
    except Exception as e:
        logger.error(f"Error cargando {file_path}: {e}")
        return set()

def load_esco_mapping(file_path: str) -> Dict[str, str]:
    mapping = {}
    try:
        with open(file_path, 'rb') as f:
            parser = ijson.parse(f)
            es_term = None
            for prefix, event, value in parser:
                if prefix.endswith("preferredLabel.es") and event == "string":
                    es_term = value.lower()
                elif prefix.endswith("preferredLabel.en") and event == "string" and es_term:
                    mapping[es_term] = value.lower()
                    es_term = None
        logger.info(f"Mapeo ESCO cargado con {len(mapping)} términos (es → en).")
        return mapping
    except Exception as e:
        logger.error(f"Error cargando mapeo ESCO: {e}")
        return {}

class BaseNLPProcessor:
    def __init__(self, language: str = 'es', mode: str = 'quick', business_unit: str = None):
        self.language = language
        self.mode = mode
        self.business_unit = business_unit
        self.nlp = model_manager.get_model(language)
        self.esco_mapping = load_esco_mapping(CATALOG_PATHS["global"]["esco"])
        self.esco_catalog = stream_dict_json(CATALOG_PATHS["global"]["esco"])
        self.skills_relax = stream_skill_db(CATALOG_PATHS["es"]["skills_relax"])
        self.skills_opportunities = load_skills_opportunities(CATALOG_PATHS["es"]["skills_opportunities"])
        self.all_skills = set().union(
            self.esco_catalog.values(),
            self.skills_relax,
            self.skills_opportunities
        )
        if business_unit:
            self._add_business_unit_skills(business_unit)
        self.normalized_skills = set(self._normalize_to_english(skill) for skill in self.all_skills if skill)
        logger.info(f"Total habilidades normalizadas: {len(self.normalized_skills)} - Ejemplos: {list(self.normalized_skills)[:5]}")
        self.phrase_matcher = self._build_phrase_matcher()
        self.skill_cache = TTLCache(maxsize=1000, ttl=3600)

    def _normalize_to_english(self, skill: str) -> str:
        return self.esco_mapping.get(skill.lower(), skill.lower())

    def _add_business_unit_skills(self, business_unit: str):
        unit_skills = {
            "TI": {"python", "django", "javascript", "sql"},
            "Marketing": {"seo", "gestión de redes sociales", "marketing digital"}
        }.get(business_unit, set())
        self.all_skills.update(unit_skills)
        logger.info(f"Añadidas {len(unit_skills)} habilidades para unidad {business_unit}: {unit_skills}")

    def _build_phrase_matcher(self) -> PhraseMatcher:
        matcher = PhraseMatcher(self.nlp.vocab, attr="LOWER")
        patterns = [self.nlp.make_doc(skill) for skill in self.normalized_skills if skill]
        matcher.add("SKILLS", patterns)
        return matcher

    @cachedmethod(lambda self: self.skill_cache)
    def extract_skills(self, text: str) -> Dict[str, List[str]]:
        doc = self.nlp(text)
        skills = {"technical": [], "soft": []}
        matches = self.phrase_matcher(doc)
        for _, start, end in matches:
            skill = doc[start:end].text.lower()
            normalized_skill = self._normalize_to_english(skill)
            if normalized_skill in self.skills_relax or normalized_skill in self.skills_opportunities:
                skills["technical"].append(normalized_skill)
            else:
                skills["soft"].append(normalized_skill)
        return {k: list(set(v)) for k, v in skills.items()}

    def analyze(self, text: str) -> Dict[str, any]:
        skills = self.extract_skills(text)
        return {
            "skills": skills,
            "total_skills": sum(len(v) for v in skills.values())
        }

class NLPProcessor:
    def __init__(self, language: str = 'es', mode: str = 'quick', business_unit: str = None):
        self.processor = BaseNLPProcessor(language, mode, business_unit)

    def analyze(self, text: str) -> Dict[str, any]:
        return self.processor.analyze(text)

# Configuraciones por fases (solo Quick md)
PHASES = [
    ("Fase 1: Modelos medianos", [
        {"mode": "quick", "desc": "Quick spaCy md (ESCO + Relax)", "business_unit": "TI"},
    ]),
]

# Función para medir recursos y analizar texto
def analyze_text(nlp: NLPProcessor, text: str, config: Dict) -> Dict:
    process = psutil.Process(os.getpid())
    mem_before = process.memory_info().rss / 1024 / 1024
    start_time = time.time()
    result = nlp.analyze(text)
    end_time = time.time()
    execution_time = end_time - start_time
    mem_after = process.memory_info().rss / 1024 / 1024
    cpu_usage = process.cpu_percent(interval=execution_time)
    logger.info(f"Resultado para '{text[:50]}...':")
    logger.info(f"  Habilidades técnicas: {result['skills']['technical']}")
    logger.info(f"  Habilidades blandas: {result['skills']['soft']}")
    logger.info(f"  Total habilidades: {result['total_skills']}")
    logger.info(f"  Tiempo: {execution_time:.4f} segundos")
    logger.info(f"  CPU: {cpu_usage:.2f}%")
    return {
        "config": config["desc"],
        "text": text,
        "skills": result["skills"],
        "total_skills": result["total_skills"],
        "execution_time": execution_time,
        "cpu_usage": cpu_usage,
        "memory_used": mem_after - mem_before
    }

# Ejecutar pruebas
results = []
for phase_name, configs in PHASES:
    logger.info(f"\n=== Iniciando {phase_name} ===")
    for config in configs:
        logger.info(f"Probando configuración: {config['desc']} (Unidad: {config['business_unit']})")
        nlp = NLPProcessor(language="es", mode=config["mode"], business_unit=config["business_unit"])
        config_results = []
        total_texts = len(texts)
        for idx, text in enumerate(texts, 1):
            logger.info(f"  Procesando texto #{idx}/{total_texts}: {text[:50]}...")
            try:
                result = analyze_text(nlp, text, config)
                config_results.append(result)
            except Exception as e:
                logger.error(f"Error procesando texto #{idx}: {e}")
                config_results.append({"text": text, "error": str(e)})
        results.append({"phase": phase_name, "config": config, "tests": config_results})

# Resumen final detallado
logger.info("\n=== Resumen Final ===")
for phase_result in results:
    config = phase_result['config']
    valid_tests = [t for t in phase_result['tests'] if "error" not in t]
    if valid_tests:
        avg_time = sum(t["execution_time"] for t in valid_tests) / len(valid_tests)
        avg_cpu = sum(t["cpu_usage"] for t in valid_tests) / len(valid_tests)
        avg_skills = sum(t["total_skills"] for t in valid_tests) / len(valid_tests)
        total_tech_skills = sum(len(t["skills"]["technical"]) for t in valid_tests)
        total_soft_skills = sum(len(t["skills"]["soft"]) for t in valid_tests)
        logger.info(f"\nConfiguración: {config['desc']} (Unidad: {config['business_unit']})")
        logger.info(f"  Tiempo promedio: {avg_time:.4f} segundos")
        logger.info(f"  Uso CPU promedio: {avg_cpu:.2f}%")
        logger.info(f"  Habilidades promedio por texto: {avg_skills:.2f}")
        logger.info(f"  Total habilidades técnicas detectadas: {total_tech_skills}")
        logger.info(f"  Total habilidades blandas detectadas: {total_soft_skills}")
        logger.info("  Detalle por texto:")
        for idx, test in enumerate(valid_tests, 1):
            logger.info(f"    Texto #{idx}: {test['text'][:50]}...")
            logger.info(f"      Técnicas: {test['skills']['technical']}")
            logger.info(f"      Blandas: {test['skills']['soft']}")
            logger.info(f"      Total: {test['total_skills']}")