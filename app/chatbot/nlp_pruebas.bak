import time
import psutil
import os
import sys
import subprocess
import spacy
import json
import logging
from typing import Dict, List, Union
from spacy.matcher import PhraseMatcher
from cachetools import cachedmethod, TTLCache
import ijson
from datetime import datetime
import logging.handlers
import torch

# Configurar logging
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
log_file = f"logs/original_nuevo_{timestamp}.log"
os.makedirs("logs", exist_ok=True)
handler = logging.handlers.RotatingFileHandler(log_file, maxBytes=10*1024*1024, backupCount=5)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[handler, logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# Importaciones condicionales para transformers
try:
    from transformers import pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    logger.warning("Transformers no disponible.")

# Rutas a los catálogos
CATALOG_PATHS = {
    "global": {"esco": "/home/pablollh/skills_data/ESCO_occup_skills.json"},
    "es": {
        "skills_relax": "/home/pablollh/skills_data/skill_db_relax_20_pretty.json",
        "skills_opportunities": "/home/pablollh/app/utilidades/catalogs/skills.json"
    }
}
COMBINED_SKILLS_FILE = "/home/pablollh/skills_data/skills_combined.json"

# Lista de textos de prueba
texts = [
    "Tengo amplia experiencia en Python, Django y liderazgo en equipos.",
    "I have strong skills in JavaScript, React, and agile methodologies.",
    "Je suis compétent en gestion de projet et en marketing digital.",
    "Ich habe Erfahrung in der Softwareentwicklung und im technischen Management.",
    "Soy CFA y tengo conocimientos avanzados en análisis financiero.",
    "He trabajado en la construcción de viviendas y supervisado proyectos de edificación.",
    "Poseo experiencia en la compra y manejo de perecederos, logística y cadena de suministro.",
    "Conozco técnicas de marketing digital, SEO y gestión de redes sociales.",
    "Experto en robótica, automatización industrial y programación en C++.",
    "Tengo formación en psicología organizacional y liderazgo de equipos.",
    "Conozco herramientas de análisis de datos como R y SQL.",
    "Experiencia en diseño gráfico, Adobe Photoshop e Illustrator.",
    "He liderado proyectos de desarrollo sostenible y energías renovables."
]

class ModelManager:
    def __init__(self):
        self.model_cache = TTLCache(maxsize=10, ttl=21600)

    def get_model(self, lang: str) -> spacy.language.Language:
        cache_key = f"{lang}_md"
        if cache_key in self.model_cache:
            return self.model_cache[cache_key]
        model_name = {
            "es": "es_core_news_md",
            "default": "xx_ent_wiki_sm"
        }.get(lang, "xx_ent_wiki_sm")
        try:
            model = spacy.load(model_name)
        except OSError:
            logger.info(f"Descargando modelo {model_name}...")
            subprocess.check_call([sys.executable, "-m", "spacy", "download", model_name])
            model = spacy.load(model_name)
        self.model_cache[cache_key] = model
        return model

model_manager = ModelManager()

class AdvancedModelManager:
    def __init__(self):
        self.ner_pipelines = {}

    def get_ner_pipeline(self, model_path: str):
        if not TRANSFORMERS_AVAILABLE:
            logger.warning("Transformers no disponible.")
            return None
        if model_path not in self.ner_pipelines:
            try:
                device = 0 if torch.cuda.is_available() else -1
                self.ner_pipelines[model_path] = pipeline("ner", model=model_path, device=device)
                logger.info(f"Cargado NER: {model_path} en {'GPU' if device == 0 else 'CPU'}")
            except Exception as e:
                logger.error(f"Error cargando NER {model_path}: {e}")
                return None
        return self.ner_pipelines[model_path]

advanced_model_manager = AdvancedModelManager()

# Funciones optimizadas para cargar y guardar catálogos
def stream_dict_json(file_path: str, key_field: str = "preferredLabel.es") -> Dict[str, str]:
    result = {}
    try:
        with open(file_path, 'rb') as f:
            parser = ijson.parse(f)
            current_key = None
            for prefix, event, value in parser:
                if event == "map_key" and not prefix:
                    current_key = value
                elif prefix.endswith(key_field) and event == "string":
                    result[current_key] = value.lower()
        logger.info(f"Cargado {file_path} con {len(result)} entradas.")
        return result
    except Exception as e:
        logger.error(f"Error cargando {file_path}: {e}")
        return {}

def stream_skill_db(file_path: str) -> set[str]:
    skills = set()
    try:
        with open(file_path, 'rb') as f:
            parser = ijson.parse(f)
            for prefix, event, value in parser:
                if prefix.endswith("skill_name") and event == "string":
                    skills.add(value.lower())
                elif prefix.endswith("full") and event == "string":
                    skills.add(value.lower())
        logger.info(f"Cargado {file_path} con {len(skills)} habilidades - Ejemplos: {list(skills)[:10]}")
        return skills
    except Exception as e:
        logger.error(f"Error cargando {file_path}: {e}")
        return set()

def load_skills_opportunities(file_path: str) -> set[str]:
    skills = set()
    try:
        with open(file_path, 'rb') as f:
            parser = ijson.parse(f)
            for prefix, event, value in parser:
                if prefix.endswith("Habilidades Técnicas.item") or prefix.endswith("Habilidades Blandas.item"):
                    if event == "string":
                        skills.add(value.lower())
        logger.info(f"Cargado {file_path} con {len(skills)} elementos.")
        return skills
    except Exception as e:
        logger.error(f"Error cargando {file_path}: {e}")
        return set()

def load_esco_mapping(file_path: str) -> Dict[str, str]:
    mapping = {}
    try:
        with open(file_path, 'rb') as f:
            parser = ijson.parse(f)
            es_term = None
            for prefix, event, value in parser:
                if prefix.endswith("preferredLabel.es") and event == "string":
                    es_term = value.lower()
                elif prefix.endswith("preferredLabel.en") and event == "string" and es_term:
                    mapping[es_term] = value.lower()
                    es_term = None
        logger.info(f"Mapeo ESCO cargado con {len(mapping)} términos (es → en).")
        return mapping
    except Exception as e:
        logger.error(f"Error cargando mapeo ESCO: {e}")
        return {}

def save_new_skills(new_skills: set[str], file_path: str):
    try:
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                existing_skills = set(json.load(f))
        else:
            existing_skills = set()
        updated_skills = existing_skills.union(new_skills)
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(list(updated_skills), f, ensure_ascii=False, indent=2)
        logger.info(f"Guardadas {len(new_skills)} nuevas habilidades en {file_path}. Total: {len(updated_skills)}")
    except Exception as e:
        logger.error(f"Error guardando habilidades en {file_path}: {e}")

class BaseNLPProcessor:
    def __init__(self, language: str = 'es', mode: str = 'quick', business_unit: str = None):
        self.language = language
        self.mode = mode
        self.business_unit = business_unit
        self.nlp = model_manager.get_model(language)
        self.esco_mapping = load_esco_mapping(CATALOG_PATHS["global"]["esco"])
        self.esco_catalog = stream_dict_json(CATALOG_PATHS["global"]["esco"])
        self.skills_relax = stream_skill_db(CATALOG_PATHS["es"]["skills_relax"])
        self.skills_opportunities = load_skills_opportunities(CATALOG_PATHS["es"]["skills_opportunities"])
        self.all_skills = set().union(
            self.esco_catalog.values(),
            self.skills_relax,
            self.skills_opportunities
        )
        if business_unit:
            self._add_business_unit_skills(business_unit)
        self.normalized_skills = set(self._normalize_to_english(skill) for skill in self.all_skills if skill)
        logger.info(f"Total habilidades normalizadas: {len(self.normalized_skills)} - Ejemplos: {list(self.normalized_skills)[:5]}")
        self.phrase_matcher = self._build_phrase_matcher()
        self.skill_cache = TTLCache(maxsize=1000, ttl=3600 if mode == 'quick' else 7200)
        self.use_full_transformers = torch.cuda.is_available() and psutil.virtual_memory().available > 8 * 1024 * 1024 * 1024  # 8GB libres
        if mode == 'deep' and TRANSFORMERS_AVAILABLE:
            self.linkedin_ner = advanced_model_manager.get_ner_pipeline("/home/pablollh/skills_data/algiraldohe_lm-ner-linkedin-skills-recognition")
            self.ihk_ner = advanced_model_manager.get_ner_pipeline("/home/pablollh/skills_data/ihk_skillner")
        else:
            self.linkedin_ner = None
            self.ihk_ner = None

    def _normalize_to_english(self, skill: str) -> str:
        return self.esco_mapping.get(skill.lower(), skill.lower())

    def _add_business_unit_skills(self, business_unit: str):
        unit_skills = {
            "TI": {"python", "django", "javascript", "sql"},
            "Marketing": {"seo", "gestión de redes sociales", "marketing digital"}
        }.get(business_unit, set())
        self.all_skills.update(unit_skills)
        logger.info(f"Añadidas {len(unit_skills)} habilidades para unidad {business_unit}: {unit_skills}")

    def _build_phrase_matcher(self) -> PhraseMatcher:
        matcher = PhraseMatcher(self.nlp.vocab, attr="LOWER")
        patterns = [self.nlp.make_doc(skill) for skill in self.normalized_skills if skill]
        matcher.add("SKILLS", patterns)
        return matcher

    @cachedmethod(lambda self: self.skill_cache)
    def extract_skills(self, text: str) -> Dict[str, List[str]]:
        doc = self.nlp(text)
        skills = {"technical": [], "soft": []}
        new_skills = set()  # Para guardar habilidades nuevas detectadas
        # Quick: PhraseMatcher con ESCO + Relax
        matches = self.phrase_matcher(doc)
        for _, start, end in matches:
            skill = doc[start:end].text.lower()
            normalized_skill = self._normalize_to_english(skill)
            if normalized_skill in self.skills_relax or normalized_skill in self.skills_opportunities:
                skills["technical"].append(normalized_skill)
            else:
                skills["soft"].append(normalized_skill)
        # Deep: Añadir transformers según recursos
        if self.mode == 'deep' and TRANSFORMERS_AVAILABLE:
            if self.use_full_transformers:
                # Modo completo: solo transformers si hay GPU y RAM
                logger.info("Usando transformers completos (GPU disponible).")
                if self.linkedin_ner:
                    entities = self.linkedin_ner(text)
                    for entity in entities:
                        if entity["entity"].startswith("SKILL"):
                            skill = entity["word"].lower()
                            normalized_skill = self._normalize_to_english(skill)
                            if normalized_skill not in self.normalized_skills:
                                new_skills.add(normalized_skill)
                            skills["technical" if normalized_skill in self.skills_relax else "soft"].append(normalized_skill)
                if self.ihk_ner:
                    entities = self.ihk_ner(text)
                    for entity in entities:
                        if entity["entity"].startswith("SKILL"):
                            skill = entity["word"].lower()
                            normalized_skill = self._normalize_to_english(skill)
                            if normalized_skill not in self.normalized_skills:
                                new_skills.add(normalized_skill)
                            skills["technical" if normalized_skill in self.skills_relax else "soft"].append(normalized_skill)
            else:
                # Modo híbrido: Quick + Transformers si solo CPU
                logger.info("Usando Quick + Transformers (sin GPU).")
                if self.linkedin_ner:
                    entities = self.linkedin_ner(text)
                    for entity in entities:
                        if entity["entity"].startswith("SKILL"):
                            skill = entity["word"].lower()
                            normalized_skill = self._normalize_to_english(skill)
                            if normalized_skill not in self.normalized_skills:
                                new_skills.add(normalized_skill)
                                skills["technical" if normalized_skill in self.skills_relax else "soft"].append(normalized_skill)
                if self.ihk_ner:
                    entities = self.ihk_ner(text)
                    for entity in entities:
                        if entity["entity"].startswith("SKILL"):
                            skill = entity["word"].lower()
                            normalized_skill = self._normalize_to_english(skill)
                            if normalized_skill not in self.normalized_skills:
                                new_skills.add(normalized_skill)
                                skills["technical" if normalized_skill in self.skills_relax else "soft"].append(normalized_skill)
        if new_skills:
            save_new_skills(new_skills, COMBINED_SKILLS_FILE)
        return {k: list(set(v)) for k, v in skills.items()}

    def analyze(self, text: str) -> Dict[str, any]:
        skills = self.extract_skills(text)
        return {
            "skills": skills,
            "total_skills": sum(len(v) for v in skills.values())
        }

class NLPProcessor:
    def __init__(self, language: str = 'es', mode: str = 'quick', business_unit: str = None):
        self.processor = BaseNLPProcessor(language, mode, business_unit)

    def analyze(self, text: str) -> Dict[str, any]:
        return self.processor.analyze(text)

# Configuraciones por fases
PHASES = [
    ("Fase 1: Modelos medianos", [
        {"mode": "quick", "desc": "Quick spaCy md (ESCO + Relax)", "business_unit": "TI"},
        {"mode": "deep", "desc": "Deep spaCy md (Conditional Transformers)", "business_unit": "TI"},
    ]),
]

# Función para medir recursos y analizar texto
def analyze_text(nlp: NLPProcessor, text: str, config: Dict) -> Dict:
    process = psutil.Process(os.getpid())
    mem_before = process.memory_info().rss / 1024 / 1024
    start_time = time.time()
    result = nlp.analyze(text)
    end_time = time.time()
    execution_time = end_time - start_time
    mem_after = process.memory_info().rss / 1024 / 1024
    cpu_usage = process.cpu_percent(interval=execution_time)
    logger.info(f"Resultado para '{text[:50]}...':")
    logger.info(f"  Habilidades técnicas: {result['skills']['technical']}")
    logger.info(f"  Habilidades blandas: {result['skills']['soft']}")
    logger.info(f"  Total habilidades: {result['total_skills']}")
    logger.info(f"  Tiempo: {execution_time:.4f} segundos")
    logger.info(f"  CPU: {cpu_usage:.2f}%")
    return {
        "config": config["desc"],
        "text": text,
        "skills": result["skills"],
        "total_skills": result["total_skills"],
        "execution_time": execution_time,
        "cpu_usage": cpu_usage,
        "memory_used": mem_after - mem_before
    }

# Ejecutar pruebas por fases
results = []
for phase_name, configs in PHASES:
    logger.info(f"\n=== Iniciando {phase_name} ===")
    for config in configs:
        logger.info(f"Probando configuración: {config['desc']} (Unidad: {config['business_unit']})")
        nlp = NLPProcessor(language="es", mode=config["mode"], business_unit=config["business_unit"])
        config_results = []
        total_texts = len(texts)
        for idx, text in enumerate(texts, 1):
            logger.info(f"  Procesando texto #{idx}/{total_texts}: {text[:50]}...")
            try:
                result = analyze_text(nlp, text, config)
                config_results.append(result)
            except Exception as e:
                logger.error(f"Error procesando texto #{idx}: {e}")
                config_results.append({"text": text, "error": str(e)})
        results.append({"phase": phase_name, "config": config, "tests": config_results})

# Resumen final detallado
logger.info("\n=== Resumen Final ===")
for phase_result in results:
    config = phase_result['config']
    valid_tests = [t for t in phase_result['tests'] if "error" not in t]
    if valid_tests:
        avg_time = sum(t["execution_time"] for t in valid_tests) / len(valid_tests)
        avg_cpu = sum(t["cpu_usage"] for t in valid_tests) / len(valid_tests)
        avg_skills = sum(t["total_skills"] for t in valid_tests) / len(valid_tests)
        total_tech_skills = sum(len(t["skills"]["technical"]) for t in valid_tests)
        total_soft_skills = sum(len(t["skills"]["soft"]) for t in valid_tests)
        logger.info(f"\nConfiguración: {config['desc']} (Unidad: {config['business_unit']})")
        logger.info(f"  Tiempo promedio: {avg_time:.4f} segundos")
        logger.info(f"  Uso CPU promedio: {avg_cpu:.2f}%")
        logger.info(f"  Habilidades promedio por texto: {avg_skills:.2f}")
        logger.info(f"  Total habilidades técnicas detectadas: {total_tech_skills}")
        logger.info(f"  Total habilidades blandas detectadas: {total_soft_skills}")
        logger.info("  Detalle por texto:")
        for idx, test in enumerate(valid_tests, 1):
            logger.info(f"    Texto #{idx}: {test['text'][:50]}...")
            logger.info(f"      Técnicas: {test['skills']['technical']}")
            logger.info(f"      Blandas: {test['skills']['soft']}")
            logger.info(f"      Total: {test['total_skills']}")