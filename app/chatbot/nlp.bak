# üìå Ubicaci√≥n en servidor: /home/pablo/app/chatbot/nlp.py


import os
import csv
import json
import logging
import asyncio
import spacy
import requests
import nltk
import unidecode
import threading
import shelve
import torch
import re
from nltk import word_tokenize, ngrams
from cachetools import cachedmethod, TTLCache
from transformers import AutoTokenizer, AutoModelForSequenceClassification

from spacy.matcher import PhraseMatcher, Matcher
from skillNer.skill_extractor_class import SkillExtractor
from skillNer.general_params import SKILL_DB
from typing import Dict, Any, List, Optional, Set, Tuple
from cachetools import TTLCache, cachedmethod
from logging.handlers import RotatingFileHandler


from app.chatbot.extractors import ESCOExtractor, NICEExtractor, ONETExtractor, CareerOneStopExtractor, unify_data, parse_esco_rdf_to_json

# ============== CONFIGURACI√ìN LOGGING ==============
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    handlers=[
        RotatingFileHandler("logs/nlp.log", maxBytes=5 * 1024 * 1024, backupCount=3),
        logging.StreamHandler()
    ]
)
"""
nlp.py - Integraci√≥n completa:
1) Parse RDF de ESCO y genera esco_from_rdf.json
2) ExternalSkillDataLoader y ESCOApiLoader para CSV/API
3) SkillDBMerger para fusionar bases en combined_skills.json
4) SkillExtractionPipeline (b√°sica) y SkillExtractorManager (avanzada)
5) NLPProcessor con la misma interfaz, usando skillNer + heur√≠sticas

Al final, en __main__, lo orquestamos:
- Parse RDF (opcional)
- run_all() para CSV,
- Merger,
- Pipeline para prueba.
- O la versi√≥n con SkillExtractorManager y NLPProcessor.

Algunas rutas y funciones son ejemplos; ajusta a tu estructura de proyecto.
"""

CONFIG = {
    "OUTPUT_DIR": "/home/pablo/skills_data",
    "COMBINED_DB_PATH": os.path.join("OUTPUT_DIR", "combined_skills.json"),
    "CACHE_DB_PATH": "/home/pablo/cache/skill_cache.db"
}

# Diccionario de modelos por idioma
MODEL_LANGUAGES = {
    "es": "es_core_news_md",
    "en": "en_core_web_md",
    # Agrega m√°s idiomas seg√∫n necesites, e.g., "fr": "fr_core_news_md"
}

def load_nlp_model(language: str = "es") -> Optional[spacy.language.Language]:
    """
    Carga un modelo spaCy para el idioma especificado con manejo de errores y fallback.

    Args:
        language (str): C√≥digo de idioma (e.g., "es" para espa√±ol, "en" para ingl√©s).

    Returns:
        Optional[spacy.language.Language]: Modelo spaCy cargado o None si falla.
    """
    model_name = MODEL_LANGUAGES.get(language, "es_core_news_md")
    fallback_model = "es_core_news_sm" if language == "es" else "en_core_web_sm"

    try:
        nlp = spacy.load(model_name)
        logger.info(f"‚úÖ Modelo spaCy '{model_name}' cargado para idioma '{language}'.")
        return nlp
    except Exception as e:
        logger.error(f"‚ùå Error cargando modelo '{model_name}': {e}")
        try:
            nlp = spacy.load(fallback_model)
            logger.info(f"‚úÖ Modelo fallback '{fallback_model}' cargado para idioma '{language}'.")
            return nlp
        except Exception as e:
            logger.error(f"‚ùå Error cargando modelo fallback '{fallback_model}': {e}")
            return None
        
def load_skill_dbs() -> dict:
    """
    Carga la base de datos de habilidades desde un archivo JSON o devuelve un diccionario vac√≠o si no existe.
    """
    db_path = CONFIG.get("COMBINED_DB_PATH", "/home/pablo/skills_data/combined_skills.json")

    if not os.path.exists(db_path):
        logger.warning(f"‚ö†Ô∏è {db_path} no existe. Devolviendo base de datos vac√≠a.")
        return {}

    try:
        with open(db_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            if isinstance(data, dict):
                logger.info(f"‚úÖ {len(data)} habilidades cargadas desde {db_path}.")
                return data
            else:
                logger.error(f"‚ùå Formato inv√°lido en {db_path}, devolviendo base vac√≠a.")
                return {}
    except Exception as e:
        logger.error(f"‚ùå Error al cargar {db_path}: {e}")
        return {}

def load_common_terms():
    """
    Carga t√©rminos comunes de habilidades t√©cnicas y generales que pueden ser utilizadas 
    en la detecci√≥n de habilidades dentro de textos.
    
    Returns:
        Dict[str, List[str]]: Un diccionario con categor√≠as de t√©rminos comunes.
    """
    return {
        "programming_languages": [
            "python", "java", "c++", "c#", "javascript", "typescript", "ruby", "go", "swift", "kotlin", "php",
            "rust", "scala", "r", "matlab", "perl", "lua", "haskell", "objective-c", "dart", "elixir"
        ],
        "frameworks": [
            "django", "flask", "spring", "react", "angular", "vue", "express", "ruby on rails", "laravel",
            "symfony", "next.js", "nuxt.js", "fastapi", "quarkus", "svelte", "ember.js"
        ],
        "databases": [
            "mysql", "postgresql", "mongodb", "sqlite", "redis", "cassandra", "mariadb", "oracle", "mssql",
            "dynamodb", "couchdb", "neo4j", "elasticsearch", "bigquery"
        ],
        "cloud_services": [
            "aws", "azure", "gcp", "google cloud", "amazon web services", "ibm cloud", "oracle cloud", "digitalocean",
            "linode", "heroku", "firebase"
        ],
        "tools": [
            "docker", "kubernetes", "terraform", "ansible", "jenkins", "git", "github actions", "gitlab ci/cd",
            "circleci", "travisci", "helm", "vagrant", "puppet", "chef", "nomad"
        ],
        "machine_learning": [
            "tensorflow", "pytorch", "scikit-learn", "keras", "xgboost", "huggingface", "openai", "spacy",
            "nltk", "fastai", "mlflow", "onnx", "gensim", "pandas", "numpy"
        ],
        "soft_skills": [
            "liderazgo", "comunicaci√≥n", "negociaci√≥n", "resoluci√≥n de problemas", "trabajo en equipo",
            "pensamiento cr√≠tico", "adaptabilidad", "gesti√≥n del tiempo", "creatividad", "inteligencia emocional"
        ]
    }
# ============== CONFIGURACI√ìN NLTK ==============
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# ============== PLACEHOLDERS Y DICCIONARIOS (para roles, business_units, etc.) ==============
# <<<--- EJEMPLOS: ajusta a tu realidad

BUSINESS_UNIT_SKILLS = {
    "huntRED¬Æ": {
        "Marketing": ["marketing digital", "seo", "sem", "branding"],
        "Ventas": ["negociaci√≥n", "pipeline de ventas", "crm", "gesti√≥n de cuentas"],
    },
    "Amigro¬Æ": {
        "Operaciones": ["mantenimiento b√°sico", "montacargas", "control de calidad"],
        "Log√≠stica": ["cadena de suministro", "almac√©n", "gesti√≥n de inventarios"],
    },
    # ...
}

def get_all_skills_for_unit(business_unit: str = "huntRED¬Æ") -> List[str]:
    """
    Dependiendo de la business_unit, retorna la lista de skills definidas en
    BUSINESS_UNIT_SKILLS o en tu base de datos real.
    """
    # <<<--- En tu caso real, podr√≠as consultar una BD o un JSON m√°s complejo
    # Por simplicidad, concateno las listas definidas en BUSINESS_UNIT_SKILLS[business_unit].
    if business_unit not in BUSINESS_UNIT_SKILLS:
        logger.warning(f"Business unit '{business_unit}' no encontrada en BUSINESS_UNIT_SKILLS.")
        return []
    skill_lists = BUSINESS_UNIT_SKILLS[business_unit].values()
    all_skills = []
    for sklist in skill_lists:
        all_skills.extend(sklist)
    return all_skills

def prioritize_interests(skills_list: List[str]) -> Dict[str, float]:
    """
    EJEMPLO: retorna un dict con cada skill y su 'peso' de 1.0.
    O podr√≠as poner una l√≥gica m√°s compleja si lo deseas.
    """
    # <<<--- Esto es un placeholder
    return {skill: 1.0 for skill in skills_list}

def get_positions_by_skills(skills: Set[str], skill_weights: Dict[str, float]) -> List[str]:
    """
    Dada una lista de skills, sugiere roles o puestos. Placeholder.
    Podr√≠as tener un mapeo 'role -> skills requeridas', y hacer un matchscore.
    """
    # <<<--- Placeholder: si detecta 'marketing digital', sugiero "Especialista en Mkt"
    suggestions = []
    if "marketing digital" in skills or "seo" in skills:
        suggestions.append("Especialista en Marketing Digital")
    if "negociaci√≥n" in skills or "crm" in skills:
        suggestions.append("Ejecutivo de Ventas")
    if "mantenimiento b√°sico" in skills:
        suggestions.append("T√©cnico de Operaciones")
    # ...
    return suggestions

def create_compatible_phrase_matcher(vocab, attr="LOWER"):
    """Crea un PhraseMatcher compatible con diferentes versiones de spaCy"""
    matcher = PhraseMatcher(vocab)
    if hasattr(matcher, 'attr'):
        matcher.attr = attr
    return matcher
# ============== MERGE DB =================
class SkillDBMerger:
    """
    Fusiona archivos <xxx>_skills.json en un solo combined_skills.json
    """
    def __init__(self, input_dir: str = "OUTPUT_DIR", output_file: str = "combined_skills.json"):
        self.input_dir = input_dir
        self.output_file = output_file
    
    def merge(self):
        combined_db = {}
        for filename in os.listdir(self.input_dir):
            if filename.endswith("_skills.json"):
                path = os.path.join(self.input_dir, filename)
                try:
                    with open(path, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        logger.info(f"Cargando {len(data)} skills desde {path}")
                        
                        for skill_id, skill_obj in data.items():
                            if skill_id not in combined_db:
                                combined_db[skill_id] = skill_obj
                            else:
                                # Combinar low_surface_forms
                                existing = combined_db[skill_id]
                                ex_lows = set(existing.get("low_surface_forms", []))
                                new_lows = set(skill_obj.get("low_surface_forms", []))
                                combined_lows = ex_lows.union(new_lows)
                                existing["low_surface_forms"] = list(combined_lows)
                except Exception as e:
                    logger.error(f"No se pudo combinar {filename}: {e}", exc_info=True)
        
        out_path = os.path.join(self.input_dir, self.output_file)
        with open(out_path, "w", encoding="utf-8") as out:
            json.dump(combined_db, out, indent=2, ensure_ascii=False)
        logger.info(f"Fusi√≥n completa. {len(combined_db)} skills totales -> {out_path}.")

# ============== EJEMPLO Pipeline (SkillExtractionPipeline) ==============
class SkillExtractionPipeline:
    """
    Pipeline b√°sico para extracci√≥n de habilidades usando skillNer con PhraseMatcher.
    Se alimenta de un archivo combined_skills.json y un modelo de spaCy.
    """
    def __init__(self,
                 combined_db_path: str = os.path.join("OUTPUT_DIR", "combined_skills.json"),
                 language_model: str = "es_core_news_md"):
        # Asegurar que la ruta use OUTPUT_DIR correctamente
        self.combined_db_path = combined_db_path

        # Cargar la base de datos de habilidades
        if not os.path.exists(self.combined_db_path):
            logger.warning(f"‚ö†Ô∏è {self.combined_db_path} no existe. Usando SKILL_DB interno.")
            self.skill_db = SKILL_DB
        else:
            try:
                with open(self.combined_db_path, "r", encoding="utf-8") as f:
                    self.skill_db = json.load(f)
                if not isinstance(self.skill_db, dict):
                    raise ValueError("El archivo JSON no contiene un diccionario v√°lido.")
                logger.info(f"‚úÖ {len(self.skill_db)} habilidades cargadas desde {self.combined_db_path}.")
            except Exception as e:
                logger.error(f"‚ùå Error cargando {self.combined_db_path}: {e}. Usando SKILL_DB.")
                self.skill_db = SKILL_DB

        # Cargar el modelo spaCy
        self.nlp = None
        try:
            self.nlp = spacy.load(language_model)
            logger.info(f"‚úÖ Modelo spaCy '{language_model}' cargado.")
        except Exception as e:
            logger.error(f"‚ùå No se pudo cargar {language_model}: {e}. Intentando fallback.")
            try:
                self.nlp = spacy.load("es_core_news_sm")
                logger.info("‚úÖ Modelo fallback 'es_core_news_sm' cargado.")
            except Exception as e:
                logger.error(f"‚ùå No se pudo cargar el modelo fallback: {e}.")

        # Inicializar SkillExtractor solo si hay un modelo spaCy v√°lido
        self.skill_extractor = None
        if self.nlp:
            try:
                self.phrase_matcher = PhraseMatcher(self.nlp.vocab)
                if hasattr(self.phrase_matcher, 'attr'):
                    self.phrase_matcher.attr = "LOWER"
                self.skill_extractor = SkillExtractor(
                    nlp=self.nlp,
                    skills_db=self.skill_db,
                    phraseMatcher=self.phrase_matcher
                )
                logger.info("‚úÖ SkillExtractor (skillNer) inicializado con √©xito (pipeline b√°sico).")
            except Exception as e:
                logger.error(f"‚ùå Error inicializando SkillExtractor: {e}", exc_info=True)
        else:
            logger.error("‚ùå No se puede inicializar SkillExtractor sin un modelo spaCy.")

    def extract_skills(self, text: str) -> List[str]:
        if not self.skill_extractor:
            logger.warning("SkillExtractor no disponible.")
            return []
        
        doc = self.nlp(text)
        res = self.skill_extractor.annotate(doc)
        detected = set()
        if "results" in res:
            for match_type in ["full_matches", "ngram_scored"]:
                if match_type in res["results"]:
                    for info in res["results"][match_type]:
                        val = info.get("doc_node_value","").strip().lower()
                        if val:
                            detected.add(val)
        return list(detected)

    def debug_extract(self, text: str):
        skills = self.extract_skills(text)
        logger.info(f"{text} -> {skills}")

# ============== AVANZADO: SkillExtractorManager + heur√≠sticas ==============
class SkillExtractorManager:
    _instance = None
    _lock = threading.Lock()

    def __init__(self, language: str = "es"):
        self.language = language
        try:
            self.skill_cache = shelve.open(CONFIG["CACHE_DB_PATH"], 'c')
        except Exception as e:
            logger.error(f"‚ùå Error abriendo cach√© en {CONFIG['CACHE_DB_PATH']}: {e}. Usando cach√© en memoria.")
            self.skill_cache = {}  # Fallback a cach√© en memoria
        self._nlp = load_nlp_model(language)
        if not self._nlp:
            logger.error("‚ùå No se pudo cargar el modelo NLP. SkillExtractorManager no iniciado.")
            self._skill_extractor = None
            return
        self._skill_db = load_skill_dbs()
        logger.info(f"üîé skill_db combinada contiene {len(self._skill_db)} √≠tems")
        self._common_terms = load_common_terms()
        self._phrase_matcher = PhraseMatcher(self._nlp.vocab)
        if hasattr(self._phrase_matcher, 'attr'): 
            self._phrase_matcher.attr = "LOWER"

        self._matcher = Matcher(self._nlp.vocab)
        try:
            self._skill_extractor = SkillExtractor(
                nlp=self._nlp,
                skills_db=self._skill_db,
                phraseMatcher=self._phrase_matcher
            )
            logger.info("‚úÖ SkillExtractor de skillNer inicializado correctamente.")
        except Exception as e:
            logger.error(f"‚ùå Error inicializando SkillExtractor: {e}", exc_info=True)
            self._skill_extractor = None
        self._prepare_spacy_patterns()

    def _prepare_spacy_patterns(self):
        for category, terms in self._common_terms.items():
            for term in terms:
                if " " not in term:
                    self._matcher.add(category, [[{"LOWER": term}]])
                else:
                    pattern = [{"LOWER": w} for w in term.split()]
                    self._matcher.add(category, [pattern])

    @property
    def ner_pipeline(self):
        if self._ner_pipeline is None:
            try:
                from transformers import pipeline
                self._ner_pipeline = pipeline("ner", model="Babelscape/wikineural-multilingual-ner")
                logger.info("‚úÖ Pipeline NER de transformers inicializado.")
            except Exception as e:
                logger.error(f"‚ùå Error inicializando pipeline NER: {e}")
                return None
        return self._ner_pipeline

    @classmethod
    def get_instance(cls, language: str = "es"):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = cls(language)
        return cls._instance

    @classmethod
    def get(cls, language: str = "es"):
        return cls.get_instance(language)

    def extract_skills(self, text: str) -> Dict[str, List[str]]:
        if not self._nlp:
            logger.warning("‚ö†Ô∏è Modelo NLP no inicializado.")
            return {"skills": []}
        try:
            skills = set()
            doc = self._nlp(text)
            if self._skill_extractor:
                try:
                    annotations = self._skill_extractor.annotate(doc)
                    if "results" in annotations:
                        for match_type in ["full_matches", "ngram_scored"]:
                            if match_type in annotations["results"]:
                                for info in annotations["results"][match_type]:
                                    skill = info.get("doc_node_value", "").strip().lower()
                                    if skill:
                                        skills.add(skill)
                except Exception as e:
                    logger.error(f"‚ùå Error usando SkillExtractor: {e}")
            if self._matcher:
                matches = self._matcher(doc)
                for match_id, start, end in matches:
                    skills.add(doc[start:end].text.lower())
            result = list(skills)
            logger.info(f"üìä Total de habilidades extra√≠das: {result}")
            return {"skills": result}
        except Exception as e:
            logger.error(f"‚ùå Error extrayendo habilidades: {e}", exc_info=True)
            return {"skills": []}
        
    def _extract_with_skillner(self, text: str) -> Set[str]:
        """Extrae habilidades usando skillNer."""
        skills = set()
        
        if not self._skill_extractor:
            return skills
            
        try:
            # Procesamos el documento con spaCy
            doc = self._nlp(text)
            
            # Aplicamos skillNer
            annotations = self._skill_extractor.annotate(doc)
            
            # Procesamos las anotaciones seg√∫n su formato
            if isinstance(annotations, dict):
                # Si tenemos resultados directos
                if "results" in annotations:
                    results = annotations["results"]
                    for match_type in ["full_matches", "ngram_scored"]:
                        if match_type in results:
                            match_data = results[match_type]
                            # Si es un diccionario indexado (antigua versi√≥n)
                            if isinstance(match_data, dict):
                                for skill_info in match_data.values():
                                    if isinstance(skill_info, dict) and "doc_node_value" in skill_info:
                                        skill_name = skill_info["doc_node_value"].strip().lower()
                                        if skill_name:
                                            skills.add(skill_name)
                            # Si es una lista (nueva versi√≥n)
                            elif isinstance(match_data, list):
                                for skill_info in match_data:
                                    if isinstance(skill_info, dict) and "doc_node_value" in skill_info:
                                        skill_name = skill_info["doc_node_value"].strip().lower()
                                        if skill_name:
                                            skills.add(skill_name)
            
            logger.info(f"üîç skillNer encontr√≥: {skills}")
        except Exception as e:
            logger.error(f"‚ùå Error en _extract_with_skillner: {e}", exc_info=True)
        
        return skills

    def _extract_with_spacy_matcher(self, text: str) -> Set[str]:
        """Extrae habilidades usando patrones personalizados con Matcher de spaCy."""
        skills = set()
        
        if not self._nlp or not self._matcher:
            return skills
        
        try:
            doc = self._nlp(text)
            matches = self._matcher(doc)
            
            for match_id, start, end in matches:
                # Obtener el nombre de la categor√≠a (programaci√≥n, framework, etc.)
                category = self._nlp.vocab.strings[match_id]
                # Obtener el texto coincidente
                span = doc[start:end]
                skill_name = span.text.lower()
                skills.add(skill_name)
            
            logger.info(f"üîç spaCy Matcher encontr√≥: {skills}")
        except Exception as e:
            logger.error(f"‚ùå Error en _extract_with_spacy_matcher: {e}")
        
        return skills

    def _extract_with_ngrams(self, text: str) -> Set[str]:
        """Detecta posibles habilidades utilizando an√°lisis de n-gramas."""
        skills = set()
        
        try:
            # Normalizar texto
            text_lower = text.lower()
            text_normalized = unidecode.unidecode(text_lower)
            
            # Tokenizar y generar n-gramas (1, 2 y 3 palabras)
            tokens = word_tokenize(text_normalized)
            candidates = []
            
            # Unigrams
            candidates.extend(tokens)
            
            # Bigrams
            if len(tokens) >= 2:
                bigrams_list = list(ngrams(tokens, 2))
                bigrams_text = [' '.join(bg) for bg in bigrams_list]
                candidates.extend(bigrams_text)
            
            # Trigrams
            if len(tokens) >= 3:
                trigrams_list = list(ngrams(tokens, 3))
                trigrams_text = [' '.join(tg) for tg in trigrams_list]
                candidates.extend(trigrams_text)
            
            # Verificar candidatos contra bases de habilidades
            for candidate in candidates:
                # 1. Verificar en el skill_db
                if candidate in self._skill_db:
                    skills.add(candidate)
                    continue
                    
                # 2. Verificar en t√©rminos comunes
                for category, terms in self._common_terms.items():
                    if candidate in terms:
                        skills.add(candidate)
                        break
            
            logger.info(f"üîç An√°lisis de n-gramas encontr√≥: {skills}")
        except Exception as e:
            logger.error(f"‚ùå Error en _extract_with_ngrams: {e}")
        
        return skills

    def _extract_with_transformers(self, text: str) -> Set[str]:
        """Extrae posibles habilidades usando un modelo de NER basado en transformers."""
        skills = set()
        
        if not self.ner_pipeline:
            return skills
        
        try:
            # Obtener entidades reconocidas
            ner_results = self.ner_pipeline(text)
            
            # Filtrar por categor√≠as relevantes (ORG, TECH, PRODUCT)
            potential_skills = []
            for item in ner_results:
                if item['entity'] in ['B-ORG', 'I-ORG', 'B-PRODUCT', 'I-PRODUCT']:
                    potential_skills.append(item['word'].lower().replace('##', ''))
            
            # Reconstruir palabras fragmentadas por tokenizaci√≥n
            i = 0
            current_skill = ""
            while i < len(potential_skills):
                if potential_skills[i].startswith('##'):
                    current_skill += potential_skills[i].replace('##', '')
                else:
                    if current_skill:
                        skills.add(current_skill)
                    current_skill = potential_skills[i]
                i += 1
            
            if current_skill:
                skills.add(current_skill)
            
            # Verificar contra diccionarios para evitar falsos positivos
            filtered_skills = set()
            for skill in skills:
                # Verificar en el skill_db o en t√©rminos comunes
                if skill in self._skill_db:
                    filtered_skills.add(skill)
                    continue
                
                for terms in self._common_terms.values():
                    if skill in terms:
                        filtered_skills.add(skill)
                        break
            
            logger.info(f"üîç NER con transformers encontr√≥: {filtered_skills}")
            return filtered_skills
        except Exception as e:
            logger.error(f"‚ùå Error en _extract_with_transformers: {e}")
        
        return skills

    def _extract_with_heuristics(self, text: str) -> Set[str]:
        """Aplica reglas heur√≠sticas espec√≠ficas para encontrar habilidades."""
        skills = set()
        
        try:
            text_lower = text.lower()
            text_normalized = unidecode.unidecode(text_lower)
            
            # Patrones para habilidades t√©cnicas espec√≠ficas
            tech_patterns = {
                r'\b(?:program|code|develop)\s+(?:in|with)?\s+([a-z\+\#]+)': 'programming_languages',
                r'\bexperien(?:ce|cia)\s+(?:with|en|con)?\s+([a-z\+\#\.]+)': 'general',
                r'\bconoc(?:er|imiento)\s+(?:de|en)?\s+([a-z\+\#\.]+)': 'general',
                r'\b(?:expert|experiencia|conocimientos)\s+(?:in|on|en|de)?\s+([a-z\+\#\.]+)': 'general',
                r'\bhabil(?:idad|idades)\s+(?:en|con|de)?\s+([a-z\+\#\.]+)': 'general',
                r'\b(?:using|utilizando|con)\s+([a-z\+\#\.]+)': 'tools',
                r'\b(?:database|bd|bbdd|base de datos)\s+([a-z\+\#\.]+)': 'databases',
                r'\b(?:framework|librer√≠a|biblioteca|library)\s+([a-z\+\#\.]+)': 'frameworks'
            }
            
            # Buscar coincidencias con los patrones
            for pattern, category in tech_patterns.items():
                matches = re.finditer(pattern, text_normalized)
                for match in matches:
                    potential_skill = match.group(1).strip()
                    
                    # Verificar si es una habilidad v√°lida (en diccionarios o t√©rminos comunes)
                    if potential_skill in self._skill_db:
                        skills.add(potential_skill)
                        continue
                    
                    # Verificar en t√©rminos comunes por categor√≠a
                    if category in self._common_terms and potential_skill in self._common_terms[category]:
                        skills.add(potential_skill)
                    elif category == 'general':
                        # Para la categor√≠a general, buscar en todas las categor√≠as
                        for terms in self._common_terms.values():
                            if potential_skill in terms:
                                skills.add(potential_skill)
                                break
            
            # Detectar versiones de lenguajes (Python 3.9, Java 11, etc.)
            version_pattern = r'\b(python|java|php|javascript|typescript|c\+\+|c\#)\s+(\d+(?:\.\d+)?)+'
            for match in re.finditer(version_pattern, text_normalized):
                lang = match.group(1)
                version = match.group(2)
                skills.add(f"{lang} {version}")
                skills.add(lang)  # Tambi√©n agregar el lenguaje sin versi√≥n
            
            # Buscar acr√≥nimos conocidos (NLP, ML, AI, etc.)
            acronyms = {
                "nlp": "natural language processing",
                "ml": "machine learning",
                "ai": "artificial intelligence",
                "dl": "deep learning",
                "cv": "computer vision",
                "api": "api development",
                "oop": "object oriented programming",
                "ci/cd": "continuous integration",
                "aws": "amazon web services",
                "gcp": "google cloud platform",
                "ui/ux": "user interface design"
            }
            
            # Buscar acr√≥nimos en el texto
            for acronym, full_term in acronyms.items():
                pattern = r'\b' + re.escape(acronym) + r'\b'
                if re.search(pattern, text_lower):
                    skills.add(acronym)
                    skills.add(full_term)
            
            logger.info(f"üîç An√°lisis heur√≠stico encontr√≥: {skills}")
        except Exception as e:
            logger.error(f"‚ùå Error en _extract_with_heuristics: {e}")
        
        return skills

    def fallback_skill_detection(self, text: str) -> Set[str]:
        """
        M√©todo fallback mejorado para cuando otros m√©todos fallan o no devuelven resultados.
        Utiliza las mismas categor√≠as cargadas en com√∫n para mantener coherencia.
        """
        skills = set()
        
        # Usar los t√©rminos ya cargados
        fallback_lists = self._common_terms
        
        # Buscar coincidencias en el texto
        text_lower = text.lower()
        for category, terms in fallback_lists.items():
            for term in terms:
                if term in text_lower:
                    skills.add(term)
        
        # Buscar versiones espec√≠ficas
        version_pattern = r'\b(python|java|javascript|react)\s+(\d+(?:\.\d+)?)\b'
        for match in re.finditer(version_pattern, text_lower):
            skills.add(f"{match.group(1)} {match.group(2)}")
        
        logger.info(f"üìå Fallback encontr√≥: {skills}")
        return skills
    
    def __del__(self):
        if hasattr(self, 'skill_cache') and isinstance(self.skill_cache, shelve.DbfilenameShelf):
            self.skill_cache.close()

# ‚úÖ Lazy Load NLPProcessor
class NLPProcessor:
    def __init__(self, language: str = "es"):
        self.language = language
        self._nlp = None
        self._matcher = None
        self._gpt_handler = None
        self._sentiment_analyzer = None
        self.gpt_cache = TTLCache(maxsize=1000, ttl=3600)

    @property
    def nlp(self):
        if self._nlp is None:
            self._nlp = load_nlp_model(self.language)
        return self._nlp

    @property
    def matcher(self):
        if self._matcher is None:
            self._matcher = Matcher(self.nlp.vocab)
        return self._matcher

    @property
    def sentiment_analyzer(self):
        if self._sentiment_analyzer is None:
            self._sentiment_analyzer = RoBertASentimentAnalyzer()
        return self._sentiment_analyzer

    async def initialize_gpt_handler(self):
        if self._gpt_handler is None:
            self._gpt_handler = await get_gpt_client()

    def set_language(self, language: str):
        self.language = language
        self.nlp = load_nlp_model(language)
        self.matcher = Matcher(self.nlp.vocab) if self.nlp else None
        logger.info(f"üîÑ Modelo NLP cambiado a: {language}")

    def define_intent_patterns(self) -> None:
        pass  # Ejemplo sin cambios

    async def analyze(self, text: str) -> dict:
        if not self.nlp:
            logger.error("No se ha cargado el modelo spaCy, devolviendo an√°lisis vac√≠o.")
            return {"intents": [], "entities": [], "sentiment": "neutral", "detected_divisions": []}
        from app.chatbot.utils import clean_text
        cleaned_text = clean_text(text)
        doc = await asyncio.to_thread(self.nlp, cleaned_text)
        entities = [(ent.text, ent.label_) for ent in doc.ents]
        matches = await asyncio.to_thread(self.matcher, doc)
        intents = [self.nlp.vocab.strings[m[0]] for m in matches]
        sentiment = await asyncio.to_thread(self.sentiment_analyzer.analyze_sentiment, cleaned_text)
        # ...
        return {
            "entities": entities,
            "intents": intents,
            "sentiment": sentiment,
            "detected_divisions": []
        }

    @cachedmethod(lambda self: self.gpt_cache)
    async def extract_skills(self, text: str, business_unit: str = "huntRED¬Æ") -> Dict[str, List[str]]:
        extractor = SkillExtractorManager.get_instance(self.language)
        if not extractor:
            logger.warning("‚ö†Ô∏è SkillExtractor no est√° inicializado")
            return {"skills": []}
        try:
            from app.chatbot.utils import clean_text
            cleaned_text = clean_text(text)
            skills_result = extractor.extract_skills(cleaned_text)
            logger.info(f"üìä Habilidades extra√≠das: {skills_result['skills']}")
            return skills_result
        except Exception as e:
            logger.error(f"‚ùå Error extrayendo habilidades: {e}", exc_info=True)
            return {"skills": []}

    def extract_interests_and_skills(self, text: str) -> dict:
        text_normalized = unidecode.unidecode(text.lower())
        skills = set()
        priorities = {}
        all_skills = get_all_skills_for_unit()

        for skill in all_skills:
            skill_normalized = unidecode.unidecode(skill.lower())
            if re.search(r'\b' + re.escape(skill_normalized) + r'\b', text_normalized):
                skills.add(skill)
                priorities[skill] = 2

        if self._nlp:
            doc = self.nlp(text)
            # ...
        prioritized_interests = prioritize_interests(list(skills))
        return {"skills": list(skills), "prioritized_skills": prioritized_interests}

    def infer_gender(self, name: str) -> str:
        GENDER_DICT = {"jose": "M", "maria": "F", "andrea": "F", "juan": "M"}
        parts = name.lower().split()
        m_count = sum(1 for p in parts if p in GENDER_DICT and GENDER_DICT[p] == "M")
        f_count = sum(1 for p in parts if p in GENDER_DICT and GENDER_DICT[p] == "F")
        return "M" if m_count > f_count else "F" if f_count > m_count else "O"

    def extract_skills_and_roles(self, text: str, business_unit: str = "huntRED¬Æ") -> dict:
        text_normalized = unidecode.unidecode(text.lower())
        skills = set()
        priorities = {}
        try:
            all_skills = get_all_skills_for_unit(business_unit)
        except Exception as e:
            logger.error(f"Error obteniendo habilidades para {business_unit}: {e}")
            all_skills = []

        for skill in all_skills:
            skill_normalized = unidecode.unidecode(skill.lower())
            if re.search(r'\b' + re.escape(skill_normalized) + r'\b', text_normalized):
                skills.add(skill)
                priorities[skill] = 2

        extractor = SkillExtractorManager.get_instance(self.language)
        if extractor:
            try:
                skill_results = extractor.extract_skills(text)
                extracted_skills = set(skill_results.get("skills", []))
                skills.update(extracted_skills)
                for skill in extracted_skills:
                    priorities[skill] = priorities.get(skill, 1)
                logger.info(f"üß† Habilidades extra√≠das por SkillExtractorManager: {extracted_skills}")
            except Exception as e:
                logger.error(f"‚ùå Error en SkillExtractorManager: {e}", exc_info=True)

        skills_list = list(skills)
        weighted_skills = prioritize_interests(skills_list) if skills_list else {}
        suggested_roles = get_positions_by_skills(skills, weighted_skills) if skills else []
        return {
            "skills": skills_list,
            "suggested_roles": suggested_roles
        }

# ============== GPT CLIENT POOL / Tabiya y Robert  ==============
# ‚úÖ Pool de clientes para GPTHandler (Lazy Load)
GPT_CLIENT_POOL_SIZE = 5
gpt_client_pool = [None] * GPT_CLIENT_POOL_SIZE
gpt_client_locks = [asyncio.Lock() for _ in range(GPT_CLIENT_POOL_SIZE)]

async def get_gpt_client():
    """Devuelve un cliente GPT disponible del pool."""
    for i in range(GPT_CLIENT_POOL_SIZE):
        async with gpt_client_locks[i]:
            if gpt_client_pool[i] is None:
                gpt_client_pool[i] = GPTHandler()
                await gpt_client_pool[i].initialize()
            return gpt_client_pool[i]
    return GPTHandler()  # Si todos est√°n ocupados, crea uno nuevo.


class TabiyaJobClassifier:
    def __init__(self):
        from tabiya_livelihoods_classifier.inference.linker import EntityLinker
        self.linker = EntityLinker()

    def classify(self, text):
        return self.linker.link_text(text)

class RoBertASentimentAnalyzer:
    def __init__(self, model_name="cardiffnlp/twitter-roberta-base-sentiment-latest"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)

    def analyze_sentiment(self, text):
        """Analiza el sentimiento del texto."""
        inputs = self.tokenizer(text, return_tensors="pt")
        with torch.no_grad():
            outputs = self.model(**inputs)
            predicted_class = torch.argmax(outputs.logits, dim=1).item()
        return ["negative", "neutral", "positive"][predicted_class]
    
# AQU√ç agregamos la funci√≥n:
def get_skill_extractor(language: str = "es"):
    """
    Devuelve una instancia √∫nica de SkillExtractorManager
    para el idioma especificado (por defecto 'es').p
    """
    return SkillExtractorManager.get_instance(language)
# Instancia global del procesador
nlp_processor = NLPProcessor(language="es")

# ============== EJEMPLO MAIN ==============
if __name__ == "__main__":
    # 1) (Opcional) Parse RDF local de ESCO
    parse_esco_rdf_to_json("esco.ttl", "esco_from_rdf.json")
    
    # 2) Extraer data desde las CLASES DE EXTRACTORS (APIs, XLS, CSV, etc.)
    # Ejemplo: instanciamos cada una seg√∫n necesitemos
    esco_ext = ESCOExtractor()
    onet_ext = ONETExtractor()
    nice_ext = NICEExtractor()
    career_ext = CareerOneStopExtractor()

    # Recogemos datos de ESCO (API) en espa√±ol
    esco_skills_data = esco_ext.get_skills(language="es", limit=5)
    esco_occ_data = esco_ext.get_occupations(language="es", limit=5)

    # O from NICE XLSX
    nice_data = nice_ext.get_skills(sheet_name="Skills")
    
    # ... y as√≠ con O*NET o CareerOneStop
    onet_data = onet_ext.get_occupations(api_key="...", count=5)
    career_data = career_ext.get_careers(api_user_id="...", api_key="...", keyword="cybersecurity", limit=5)

    # 3) Unificamos los datos en un solo formato (unify_data)
    #    Por ahora, ejemplo con ESCO + NICE
    unified = unify_data(esco_skills_data, esco_occ_data, nice_data)
    logger.info(f"Datos unificados: {len(unified)}")

    # 4) (Opcional) Guardar estos datos en un .json (similar a "esco_skills.json"), 
    #    o prepara un "merger" para combinarlos con otras fuentes
    #    -> En caso de que quieras que generen un archivo tipo "ESCO_API_skills.json", etc.

    # EJEMPLO r√°pido: guardamos un 'esco_api_skills.json' con lo que unificamos
    with open(os.path.join(CONFIG["OUTPUT_DIR"], "esco_api_skills.json"), "w", encoding="utf-8") as out:
        json.dump({f"gen_{i}": item for i, item in enumerate(unified)}, out, indent=2, ensure_ascii=False)

    # 5) Fusionamos con el resto de bases para tener un combined_skills.json
    merger = SkillDBMerger(CONFIG["OUTPUT_DIR"], "combined_skills.json")
    merger.merge()

    # 6) Probar pipeline b√°sico
    pipeline = SkillExtractionPipeline(combined_db_path=CONFIG["COMBINED_DB_PATH"])
    texts = [
        "Busco un desarrollador con experiencia en Python, Django y machine learning.",
        "Me interesa contratar a alguien con habilidades en Java, React y gesti√≥n de proyectos."
    ]
    for txt in texts:
        print(f"[Pipeline B√°sico] {txt} -> {pipeline.extract_skills(txt)}")

    # 7) Probar extractor avanzado
    advanced_extractor = SkillExtractorManager.get_instance("es")
    for txt in texts:
        print(f"[Avanzado] {txt} -> {advanced_extractor.extract_skills(txt)['skills']}")

    # 8) Probar NLPProcessor
    nlp_proc = NLPProcessor(language="es")
    async def test_nlp():
        for txt in texts:
            res = await nlp_proc.extract_skills(txt)
            print(f"[NLPProcessor] {txt} -> {res['skills']}")
    asyncio.run(test_nlp())





    _______________
    # /home/pablo/app/chatbot/nlp.py
import time
import psutil
import os
import sys
import subprocess
import spacy
import json
import logging
from typing import Dict, List, Optional, Set, Union, Literal
from cachetools import TTLCache
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from langdetect import detect
from threading import Lock
from datetime import datetime

# Importaciones condicionales
try:
    from deep_translator import GoogleTranslator
    TRANSLATOR_AVAILABLE = True
except ImportError:
    TRANSLATOR_AVAILABLE = False

try:
    from transformers import pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

logger = logging.getLogger(__name__)
log_file = '/home/pablo/logs/nlp.log'
try:
    handler = logging.FileHandler(log_file)
    handler.setFormatter(logging.Formatter('%(asctime)s %(levelname)s %(name)s %(message)s'))
    logger.addHandler(handler)
except PermissionError:
    logger.addHandler(logging.StreamHandler(sys.stdout))
    logger.warning(f"No se pudo escribir en {log_file}. Usando stdout.")

# Modelos spaCy
MODEL_LANGUAGES = {
    "es": {"sm": "es_core_news_sm", "md": "es_core_news_md", "lg": "es_core_news_lg"},
    "en": {"sm": "en_core_web_sm", "md": "en_core_web_md", "lg": "en_core_web_lg"},
    "fr": {"sm": "fr_core_news_sm", "md": "fr_core_news_md", "lg": "fr_core_news_lg"},
    "default": {"sm": "xx_ent_wiki_sm", "md": "xx_ent_wiki_sm", "lg": "xx_ent_wiki_sm"}
}

# Directorios de cat√°logos
CATALOG_BASE_PATH = "/home/pablo/skills_data"
UTILITIES_CATALOG_PATH = "/home/pablo/app/utilidades/catalogs"

CATALOGS = {
    "global": {"esco": os.path.join(CATALOG_BASE_PATH, "ESCO_occup_skills.json")},
    "es": {
        "occupations": os.path.join(CATALOG_BASE_PATH, "occupations_es.json"),
        "skills": os.path.join(CATALOG_BASE_PATH, "skills_es.json"),
        "skills_relax": os.path.join(CATALOG_BASE_PATH, "skill_db_relax_20.json"),
        "skills_opportunities": os.path.join(UTILITIES_CATALOG_PATH, "skills.json")
    }
}

# Configuraci√≥n global
MAX_SKILLS = 20000  # Aumentado para cubrir m√°s casos, ajustable seg√∫n RAM
USE_TABIYA = False  # Cambia a True si integras Tabiya
json_lock = Lock()
nltk.download('vader_lexicon', quiet=True)
SIA = SentimentIntensityAnalyzer()

class CatalogManager:
    def __init__(self, catalogs: Dict = CATALOGS):
        self.catalogs = catalogs
        self.cached_data = TTLCache(maxsize=10, ttl=21600)  # 6 horas de cach√©

    def get_catalog_path(self, category: str, catalog_name: str) -> str:
        if category in self.catalogs and catalog_name in self.catalogs[category]:
            return self.catalogs[category][catalog_name]
        raise ValueError(f"Cat√°logo {catalog_name} no encontrado en categor√≠a {category}")

    def load_catalog(self, category: str, catalog_name: str) -> Union[Dict, List]:
        path = self.get_catalog_path(category, catalog_name)
        cache_key = f"{category}_{catalog_name}"
        if cache_key in self.cached_data:
            return self.cached_data[cache_key]
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            self.cached_data[cache_key] = data
            logger.info(f"Cat√°logo cargado: {path}")
            return data
        except Exception as e:
            logger.error(f"Error cargando cat√°logo {category}/{catalog_name}: {e}")
            return {}

    def get_all_skills(self, categories_and_catalogs: List[tuple], max_skills: int = MAX_SKILLS) -> Set[str]:
        skills = set()
        
        def extract_skills(data, skills_set: Set[str]) -> bool:
            if len(skills_set) >= max_skills:
                return True
            if isinstance(data, dict):
                for key, value in data.items():
                    if key in ["Habilidades T√©cnicas", "Habilidades Blandas", "Certificaciones", "Herramientas"]:
                        if isinstance(value, list):
                            skills_set.update(s.lower() for s in value if isinstance(s, str))
                    elif extract_skills(value, skills_set):
                        return True
            elif isinstance(data, list):
                for item in data:
                    if extract_skills(item, skills_set):
                        return True
            elif isinstance(data, str):
                skills_set.add(data.lower())
            if len(skills_set) >= max_skills:
                logger.warning(f"L√≠mite de {max_skills} habilidades alcanzado")
                return True
            return False

        for category, catalog_name in categories_and_catalogs:
            catalog = self.load_catalog(category, catalog_name)
            if not catalog:
                continue
            if extract_skills(catalog, skills):
                break

        logger.info(f"Total habilidades cargadas: {len(skills)}")
        return skills

class ModelManager:
    def __init__(self):
        self.spacy_cache = TTLCache(maxsize=5, ttl=21600)
        self.transformer_cache = TTLCache(maxsize=3, ttl=21600)

    def get_spacy_model(self, lang: str, size: str = 'sm') -> spacy.language.Language:
        key = f"{lang}_{size}"
        if key in self.spacy_cache:
            return self.spacy_cache[key]
        model_name = MODEL_LANGUAGES.get(lang, MODEL_LANGUAGES["default"]).get(size, "xx_ent_wiki_sm")
        try:
            model = spacy.load(model_name)
        except OSError:
            logger.info(f"Descargando modelo {model_name}...")
            subprocess.check_call([sys.executable, "-m", "spacy", "download", model_name])
            model = spacy.load(model_name)
        self.spacy_cache[key] = model
        return model

    def get_transformer_pipeline(self, task: str, model_path: str) -> Optional[pipeline]:
        if not TRANSFORMERS_AVAILABLE:
            return None
        key = f"{task}_{model_path}"
        if key in self.transformer_cache:
            return self.transformer_cache[key]
        try:
            pipe = pipeline(task, model=model_path, device=-1)
            self.transformer_cache[key] = pipe
            logger.info(f"Transformer cargado: {model_path}")
            return pipe
        except Exception as e:
            logger.error(f"Error cargando transformer {model_path}: {e}")
            return None

class NLPProcessor:
    def __init__(self, language: str = 'es', mode: Literal['candidate', 'opportunity'] = 'opportunity', analysis_depth: Literal['quick', 'deep'] = 'quick'):
        self.language = language
        self.mode = mode
        self.analysis_depth = analysis_depth
        self.catalog_mgr = CatalogManager()
        self.model_mgr = ModelManager()
        self.nlp = self.model_mgr.get_spacy_model(language, 'sm' if analysis_depth == 'quick' else 'md')
        
        self.catalog_sources = [
            ("es", "skills"),
            ("es", "occupations"),
            ("es", "skills_relax"),
            ("es", "skills_opportunities"),
            ("global", "esco")
        ]
        
        self.skills = self.catalog_mgr.get_all_skills(self.catalog_sources, max_skills=MAX_SKILLS)
        self.phrase_matcher = self._build_phrase_matcher()
        self.sentiment_analyzer = SIA if analysis_depth == 'quick' or not TRANSFORMERS_AVAILABLE else self.model_mgr.get_transformer_pipeline('sentiment-analysis', 'cardiffnlp/twitter-roberta-base-sentiment-latest')
        self.translator = GoogleTranslator(source='auto', target='es') if TRANSLATOR_AVAILABLE and analysis_depth == 'deep' else None
        
        self._log_resources("Inicializaci√≥n completada")

    def _build_phrase_matcher(self) -> spacy.matcher.PhraseMatcher:
        matcher = spacy.matcher.PhraseMatcher(self.nlp.vocab, attr='LOWER')
        patterns = [self.nlp.make_doc(skill) for skill in self.skills]
        matcher.add("SKILL", patterns)
        return matcher

    def _log_resources(self, stage: str) -> None:
        process = psutil.Process(os.getpid())
        logger.info(f"{stage} - CPU: {process.cpu_percent(interval=None):.2f}%, Memoria: {process.memory_info().rss / 1024 / 1024:.2f} MB")

    def extract_skills(self, text: str) -> Dict[str, List[str]]:
        doc = self.nlp(text.lower())
        skills = {"technical": [], "soft": [], "certifications": [], "tools": []}
        matches = self.phrase_matcher(doc)
        for _, start, end in matches:
            skill = doc[start:end].text
            self._classify_skill(skill, skills)
        
        if self.analysis_depth == 'deep' and self.translator and detect(text) != 'es':
            try:
                translated = self.translator.translate(text)
                trans_doc = self.nlp(translated.lower())
                trans_matches = self.phrase_matcher(trans_doc)
                for _, start, end in trans_matches:
                    skill = trans_doc[start:end].text
                    self._classify_skill(skill, skills)
            except Exception as e:
                logger.error(f"Error en traducci√≥n: {e}")
        
        return {k: list(set(v)) for k, v in skills.items()}

    def _classify_skill(self, skill: str, skills_dict: Dict[str, List[str]]) -> None:
        skill_lower = skill.lower()
        for category, catalog_name in self.catalog_sources:
            catalog = self.catalog_mgr.load_catalog(category, catalog_name)
            if not catalog:
                continue
            found = False
            
            def check_skills(data, target: str) -> Optional[str]:
                if isinstance(data, dict):
                    for key, value in data.items():
                        if key in ["Habilidades T√©cnicas", "Habilidades Blandas", "Certificaciones", "Herramientas"]:
                            if isinstance(value, list) and target in [s.lower() for s in value if isinstance(s, str)]:
                                return key
                        elif result := check_skills(value, target):
                            return result
                elif isinstance(data, list):
                    if target in [s.lower() for s in data if isinstance(s, str)]:
                        return "technical"  # Default si no hay categor√≠a expl√≠cita
                elif isinstance(data, str) and target == data.lower():
                    return "technical"
                return None

            category_key = check_skills(catalog, skill_lower)
            if category_key:
                key_map = {
                    "Habilidades T√©cnicas": "technical",
                    "Habilidades Blandas": "soft",
                    "Certificaciones": "certifications",
                    "Herramientas": "tools",
                    "technical": "technical"
                }
                skills_dict[key_map[category_key]].append(skill_lower)
                found = True
                break
            if found:
                break

    def analyze(self, text: str) -> Dict[str, any]:
        start_time = time.time()
        self._log_resources("Inicio an√°lisis")
        skills = self.extract_skills(text)
        sentiment = self.get_sentiment(text)
        result = {"skills": skills, "sentiment": sentiment["label"], "sentiment_score": sentiment["score"]}
        exec_time = time.time() - start_time
        self._log_resources(f"Fin an√°lisis (Tiempo: {exec_time:.2f}s)")
        return result

    def get_sentiment(self, text: str) -> Dict[str, Union[str, float]]:
        if not self.sentiment_analyzer:
            return {"label": "neutral", "score": 0.0}
        try:
            if self.analysis_depth == 'quick':
                scores = SIA.polarity_scores(text)
                compound = scores['compound']
                label = "positive" if compound >= 0.05 else "negative" if compound <= -0.05 else "neutral"
                return {"label": label, "score": abs(compound)}
            result = self.sentiment_analyzer(text[:512])[0]
            return {"label": result["label"], "score": result["score"]}
        except Exception as e:
            logger.error(f"Error en an√°lisis de sentimiento: {e}")
            return {"label": "neutral", "score": 0.0}

nlp_processor = NLPProcessor()

if __name__ == "__main__":
    sample_text = "Tengo experiencia en Python, liderazgo y gesti√≥n de proyectos."
    result = nlp_processor.analyze(sample_text)
    print(json.dumps(result, ensure_ascii=False, indent=2))