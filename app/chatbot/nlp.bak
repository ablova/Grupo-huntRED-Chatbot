# üìå Ubicaci√≥n en servidor: /home/pablollh/app/chatbot/nlp.py


import os
import csv
import json
import logging
import asyncio
import spacy
import requests
import nltk
import unidecode
import threading
import shelve
import torch
import re
from nltk import word_tokenize, ngrams
from cachetools import cachedmethod, TTLCache
from transformers import AutoTokenizer, AutoModelForSequenceClassification

from spacy.matcher import PhraseMatcher, Matcher
from skillNer.skill_extractor_class import SkillExtractor
from skillNer.general_params import SKILL_DB
from typing import Dict, Any, List, Optional, Set, Tuple
from cachetools import TTLCache, cachedmethod
from logging.handlers import RotatingFileHandler


from app.chatbot.extractors import ESCOExtractor, NICEExtractor, ONETExtractor, CareerOneStopExtractor, unify_data, parse_esco_rdf_to_json

# ============== CONFIGURACI√ìN LOGGING ==============
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    handlers=[
        RotatingFileHandler("logs/nlp.log", maxBytes=5 * 1024 * 1024, backupCount=3),
        logging.StreamHandler()
    ]
)
"""
nlp.py - Integraci√≥n completa:
1) Parse RDF de ESCO y genera esco_from_rdf.json
2) ExternalSkillDataLoader y ESCOApiLoader para CSV/API
3) SkillDBMerger para fusionar bases en combined_skills.json
4) SkillExtractionPipeline (b√°sica) y SkillExtractorManager (avanzada)
5) NLPProcessor con la misma interfaz, usando skillNer + heur√≠sticas

Al final, en __main__, lo orquestamos:
- Parse RDF (opcional)
- run_all() para CSV,
- Merger,
- Pipeline para prueba.
- O la versi√≥n con SkillExtractorManager y NLPProcessor.

Algunas rutas y funciones son ejemplos; ajusta a tu estructura de proyecto.
"""

CONFIG = {
    "OUTPUT_DIR": "/home/pablollh/skills_data",
    "COMBINED_DB_PATH": os.path.join("OUTPUT_DIR", "combined_skills.json"),
    "CACHE_DB_PATH": "/home/pablollh/cache/skill_cache.db"
}

# Diccionario de modelos por idioma
MODEL_LANGUAGES = {
    "es": "es_core_news_md",
    "en": "en_core_web_md",
    # Agrega m√°s idiomas seg√∫n necesites, e.g., "fr": "fr_core_news_md"
}

def load_nlp_model(language: str = "es") -> Optional[spacy.language.Language]:
    """
    Carga un modelo spaCy para el idioma especificado con manejo de errores y fallback.

    Args:
        language (str): C√≥digo de idioma (e.g., "es" para espa√±ol, "en" para ingl√©s).

    Returns:
        Optional[spacy.language.Language]: Modelo spaCy cargado o None si falla.
    """
    model_name = MODEL_LANGUAGES.get(language, "es_core_news_md")
    fallback_model = "es_core_news_sm" if language == "es" else "en_core_web_sm"

    try:
        nlp = spacy.load(model_name)
        logger.info(f"‚úÖ Modelo spaCy '{model_name}' cargado para idioma '{language}'.")
        return nlp
    except Exception as e:
        logger.error(f"‚ùå Error cargando modelo '{model_name}': {e}")
        try:
            nlp = spacy.load(fallback_model)
            logger.info(f"‚úÖ Modelo fallback '{fallback_model}' cargado para idioma '{language}'.")
            return nlp
        except Exception as e:
            logger.error(f"‚ùå Error cargando modelo fallback '{fallback_model}': {e}")
            return None
        
def load_skill_dbs() -> dict:
    """
    Carga la base de datos de habilidades desde un archivo JSON o devuelve un diccionario vac√≠o si no existe.
    """
    db_path = CONFIG.get("COMBINED_DB_PATH", "/home/pablollh/skills_data/combined_skills.json")

    if not os.path.exists(db_path):
        logger.warning(f"‚ö†Ô∏è {db_path} no existe. Devolviendo base de datos vac√≠a.")
        return {}

    try:
        with open(db_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            if isinstance(data, dict):
                logger.info(f"‚úÖ {len(data)} habilidades cargadas desde {db_path}.")
                return data
            else:
                logger.error(f"‚ùå Formato inv√°lido en {db_path}, devolviendo base vac√≠a.")
                return {}
    except Exception as e:
        logger.error(f"‚ùå Error al cargar {db_path}: {e}")
        return {}

def load_common_terms():
    """
    Carga t√©rminos comunes de habilidades t√©cnicas y generales que pueden ser utilizadas 
    en la detecci√≥n de habilidades dentro de textos.
    
    Returns:
        Dict[str, List[str]]: Un diccionario con categor√≠as de t√©rminos comunes.
    """
    return {
        "programming_languages": [
            "python", "java", "c++", "c#", "javascript", "typescript", "ruby", "go", "swift", "kotlin", "php",
            "rust", "scala", "r", "matlab", "perl", "lua", "haskell", "objective-c", "dart", "elixir"
        ],
        "frameworks": [
            "django", "flask", "spring", "react", "angular", "vue", "express", "ruby on rails", "laravel",
            "symfony", "next.js", "nuxt.js", "fastapi", "quarkus", "svelte", "ember.js"
        ],
        "databases": [
            "mysql", "postgresql", "mongodb", "sqlite", "redis", "cassandra", "mariadb", "oracle", "mssql",
            "dynamodb", "couchdb", "neo4j", "elasticsearch", "bigquery"
        ],
        "cloud_services": [
            "aws", "azure", "gcp", "google cloud", "amazon web services", "ibm cloud", "oracle cloud", "digitalocean",
            "linode", "heroku", "firebase"
        ],
        "tools": [
            "docker", "kubernetes", "terraform", "ansible", "jenkins", "git", "github actions", "gitlab ci/cd",
            "circleci", "travisci", "helm", "vagrant", "puppet", "chef", "nomad"
        ],
        "machine_learning": [
            "tensorflow", "pytorch", "scikit-learn", "keras", "xgboost", "huggingface", "openai", "spacy",
            "nltk", "fastai", "mlflow", "onnx", "gensim", "pandas", "numpy"
        ],
        "soft_skills": [
            "liderazgo", "comunicaci√≥n", "negociaci√≥n", "resoluci√≥n de problemas", "trabajo en equipo",
            "pensamiento cr√≠tico", "adaptabilidad", "gesti√≥n del tiempo", "creatividad", "inteligencia emocional"
        ]
    }
# ============== CONFIGURACI√ìN NLTK ==============
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# ============== PLACEHOLDERS Y DICCIONARIOS (para roles, business_units, etc.) ==============
# <<<--- EJEMPLOS: ajusta a tu realidad

BUSINESS_UNIT_SKILLS = {
    "huntRED¬Æ": {
        "Marketing": ["marketing digital", "seo", "sem", "branding"],
        "Ventas": ["negociaci√≥n", "pipeline de ventas", "crm", "gesti√≥n de cuentas"],
    },
    "Amigro¬Æ": {
        "Operaciones": ["mantenimiento b√°sico", "montacargas", "control de calidad"],
        "Log√≠stica": ["cadena de suministro", "almac√©n", "gesti√≥n de inventarios"],
    },
    # ...
}

def get_all_skills_for_unit(business_unit: str = "huntRED¬Æ") -> List[str]:
    """
    Dependiendo de la business_unit, retorna la lista de skills definidas en
    BUSINESS_UNIT_SKILLS o en tu base de datos real.
    """
    # <<<--- En tu caso real, podr√≠as consultar una BD o un JSON m√°s complejo
    # Por simplicidad, concateno las listas definidas en BUSINESS_UNIT_SKILLS[business_unit].
    if business_unit not in BUSINESS_UNIT_SKILLS:
        logger.warning(f"Business unit '{business_unit}' no encontrada en BUSINESS_UNIT_SKILLS.")
        return []
    skill_lists = BUSINESS_UNIT_SKILLS[business_unit].values()
    all_skills = []
    for sklist in skill_lists:
        all_skills.extend(sklist)
    return all_skills

def prioritize_interests(skills_list: List[str]) -> Dict[str, float]:
    """
    EJEMPLO: retorna un dict con cada skill y su 'peso' de 1.0.
    O podr√≠as poner una l√≥gica m√°s compleja si lo deseas.
    """
    # <<<--- Esto es un placeholder
    return {skill: 1.0 for skill in skills_list}

def get_positions_by_skills(skills: Set[str], skill_weights: Dict[str, float]) -> List[str]:
    """
    Dada una lista de skills, sugiere roles o puestos. Placeholder.
    Podr√≠as tener un mapeo 'role -> skills requeridas', y hacer un matchscore.
    """
    # <<<--- Placeholder: si detecta 'marketing digital', sugiero "Especialista en Mkt"
    suggestions = []
    if "marketing digital" in skills or "seo" in skills:
        suggestions.append("Especialista en Marketing Digital")
    if "negociaci√≥n" in skills or "crm" in skills:
        suggestions.append("Ejecutivo de Ventas")
    if "mantenimiento b√°sico" in skills:
        suggestions.append("T√©cnico de Operaciones")
    # ...
    return suggestions

def create_compatible_phrase_matcher(vocab, attr="LOWER"):
    """Crea un PhraseMatcher compatible con diferentes versiones de spaCy"""
    matcher = PhraseMatcher(vocab)
    if hasattr(matcher, 'attr'):
        matcher.attr = attr
    return matcher
# ============== MERGE DB =================
class SkillDBMerger:
    """
    Fusiona archivos <xxx>_skills.json en un solo combined_skills.json
    """
    def __init__(self, input_dir: str = "OUTPUT_DIR", output_file: str = "combined_skills.json"):
        self.input_dir = input_dir
        self.output_file = output_file
    
    def merge(self):
        combined_db = {}
        for filename in os.listdir(self.input_dir):
            if filename.endswith("_skills.json"):
                path = os.path.join(self.input_dir, filename)
                try:
                    with open(path, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        logger.info(f"Cargando {len(data)} skills desde {path}")
                        
                        for skill_id, skill_obj in data.items():
                            if skill_id not in combined_db:
                                combined_db[skill_id] = skill_obj
                            else:
                                # Combinar low_surface_forms
                                existing = combined_db[skill_id]
                                ex_lows = set(existing.get("low_surface_forms", []))
                                new_lows = set(skill_obj.get("low_surface_forms", []))
                                combined_lows = ex_lows.union(new_lows)
                                existing["low_surface_forms"] = list(combined_lows)
                except Exception as e:
                    logger.error(f"No se pudo combinar {filename}: {e}", exc_info=True)
        
        out_path = os.path.join(self.input_dir, self.output_file)
        with open(out_path, "w", encoding="utf-8") as out:
            json.dump(combined_db, out, indent=2, ensure_ascii=False)
        logger.info(f"Fusi√≥n completa. {len(combined_db)} skills totales -> {out_path}.")

# ============== EJEMPLO Pipeline (SkillExtractionPipeline) ==============
class SkillExtractionPipeline:
    """
    Pipeline b√°sico para extracci√≥n de habilidades usando skillNer con PhraseMatcher.
    Se alimenta de un archivo combined_skills.json y un modelo de spaCy.
    """
    def __init__(self,
                 combined_db_path: str = os.path.join("OUTPUT_DIR", "combined_skills.json"),
                 language_model: str = "es_core_news_md"):
        # Asegurar que la ruta use OUTPUT_DIR correctamente
        self.combined_db_path = combined_db_path

        # Cargar la base de datos de habilidades
        if not os.path.exists(self.combined_db_path):
            logger.warning(f"‚ö†Ô∏è {self.combined_db_path} no existe. Usando SKILL_DB interno.")
            self.skill_db = SKILL_DB
        else:
            try:
                with open(self.combined_db_path, "r", encoding="utf-8") as f:
                    self.skill_db = json.load(f)
                if not isinstance(self.skill_db, dict):
                    raise ValueError("El archivo JSON no contiene un diccionario v√°lido.")
                logger.info(f"‚úÖ {len(self.skill_db)} habilidades cargadas desde {self.combined_db_path}.")
            except Exception as e:
                logger.error(f"‚ùå Error cargando {self.combined_db_path}: {e}. Usando SKILL_DB.")
                self.skill_db = SKILL_DB

        # Cargar el modelo spaCy
        self.nlp = None
        try:
            self.nlp = spacy.load(language_model)
            logger.info(f"‚úÖ Modelo spaCy '{language_model}' cargado.")
        except Exception as e:
            logger.error(f"‚ùå No se pudo cargar {language_model}: {e}. Intentando fallback.")
            try:
                self.nlp = spacy.load("es_core_news_sm")
                logger.info("‚úÖ Modelo fallback 'es_core_news_sm' cargado.")
            except Exception as e:
                logger.error(f"‚ùå No se pudo cargar el modelo fallback: {e}.")

        # Inicializar SkillExtractor solo si hay un modelo spaCy v√°lido
        self.skill_extractor = None
        if self.nlp:
            try:
                self.phrase_matcher = PhraseMatcher(self.nlp.vocab)
                if hasattr(self.phrase_matcher, 'attr'):
                    self.phrase_matcher.attr = "LOWER"
                self.skill_extractor = SkillExtractor(
                    nlp=self.nlp,
                    skills_db=self.skill_db,
                    phraseMatcher=self.phrase_matcher
                )
                logger.info("‚úÖ SkillExtractor (skillNer) inicializado con √©xito (pipeline b√°sico).")
            except Exception as e:
                logger.error(f"‚ùå Error inicializando SkillExtractor: {e}", exc_info=True)
        else:
            logger.error("‚ùå No se puede inicializar SkillExtractor sin un modelo spaCy.")

    def extract_skills(self, text: str) -> List[str]:
        if not self.skill_extractor:
            logger.warning("SkillExtractor no disponible.")
            return []
        
        doc = self.nlp(text)
        res = self.skill_extractor.annotate(doc)
        detected = set()
        if "results" in res:
            for match_type in ["full_matches", "ngram_scored"]:
                if match_type in res["results"]:
                    for info in res["results"][match_type]:
                        val = info.get("doc_node_value","").strip().lower()
                        if val:
                            detected.add(val)
        return list(detected)

    def debug_extract(self, text: str):
        skills = self.extract_skills(text)
        logger.info(f"{text} -> {skills}")

# ============== AVANZADO: SkillExtractorManager + heur√≠sticas ==============
class SkillExtractorManager:
    _instance = None
    _lock = threading.Lock()

    def __init__(self, language: str = "es"):
        self.language = language
        try:
            self.skill_cache = shelve.open(CONFIG["CACHE_DB_PATH"], 'c')
        except Exception as e:
            logger.error(f"‚ùå Error abriendo cach√© en {CONFIG['CACHE_DB_PATH']}: {e}. Usando cach√© en memoria.")
            self.skill_cache = {}  # Fallback a cach√© en memoria
        self._nlp = load_nlp_model(language)
        if not self._nlp:
            logger.error("‚ùå No se pudo cargar el modelo NLP. SkillExtractorManager no iniciado.")
            self._skill_extractor = None
            return
        self._skill_db = load_skill_dbs()
        logger.info(f"üîé skill_db combinada contiene {len(self._skill_db)} √≠tems")
        self._common_terms = load_common_terms()
        self._phrase_matcher = PhraseMatcher(self._nlp.vocab)
        if hasattr(self._phrase_matcher, 'attr'): 
            self._phrase_matcher.attr = "LOWER"

        self._matcher = Matcher(self._nlp.vocab)
        try:
            self._skill_extractor = SkillExtractor(
                nlp=self._nlp,
                skills_db=self._skill_db,
                phraseMatcher=self._phrase_matcher
            )
            logger.info("‚úÖ SkillExtractor de skillNer inicializado correctamente.")
        except Exception as e:
            logger.error(f"‚ùå Error inicializando SkillExtractor: {e}", exc_info=True)
            self._skill_extractor = None
        self._prepare_spacy_patterns()

    def _prepare_spacy_patterns(self):
        for category, terms in self._common_terms.items():
            for term in terms:
                if " " not in term:
                    self._matcher.add(category, [[{"LOWER": term}]])
                else:
                    pattern = [{"LOWER": w} for w in term.split()]
                    self._matcher.add(category, [pattern])

    @property
    def ner_pipeline(self):
        if self._ner_pipeline is None:
            try:
                from transformers import pipeline
                self._ner_pipeline = pipeline("ner", model="Babelscape/wikineural-multilingual-ner")
                logger.info("‚úÖ Pipeline NER de transformers inicializado.")
            except Exception as e:
                logger.error(f"‚ùå Error inicializando pipeline NER: {e}")
                return None
        return self._ner_pipeline

    @classmethod
    def get_instance(cls, language: str = "es"):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = cls(language)
        return cls._instance

    @classmethod
    def get(cls, language: str = "es"):
        return cls.get_instance(language)

    def extract_skills(self, text: str) -> Dict[str, List[str]]:
        if not self._nlp:
            logger.warning("‚ö†Ô∏è Modelo NLP no inicializado.")
            return {"skills": []}
        try:
            skills = set()
            doc = self._nlp(text)
            if self._skill_extractor:
                try:
                    annotations = self._skill_extractor.annotate(doc)
                    if "results" in annotations:
                        for match_type in ["full_matches", "ngram_scored"]:
                            if match_type in annotations["results"]:
                                for info in annotations["results"][match_type]:
                                    skill = info.get("doc_node_value", "").strip().lower()
                                    if skill:
                                        skills.add(skill)
                except Exception as e:
                    logger.error(f"‚ùå Error usando SkillExtractor: {e}")
            if self._matcher:
                matches = self._matcher(doc)
                for match_id, start, end in matches:
                    skills.add(doc[start:end].text.lower())
            result = list(skills)
            logger.info(f"üìä Total de habilidades extra√≠das: {result}")
            return {"skills": result}
        except Exception as e:
            logger.error(f"‚ùå Error extrayendo habilidades: {e}", exc_info=True)
            return {"skills": []}
        
    def _extract_with_skillner(self, text: str) -> Set[str]:
        """Extrae habilidades usando skillNer."""
        skills = set()
        
        if not self._skill_extractor:
            return skills
            
        try:
            # Procesamos el documento con spaCy
            doc = self._nlp(text)
            
            # Aplicamos skillNer
            annotations = self._skill_extractor.annotate(doc)
            
            # Procesamos las anotaciones seg√∫n su formato
            if isinstance(annotations, dict):
                # Si tenemos resultados directos
                if "results" in annotations:
                    results = annotations["results"]
                    for match_type in ["full_matches", "ngram_scored"]:
                        if match_type in results:
                            match_data = results[match_type]
                            # Si es un diccionario indexado (antigua versi√≥n)
                            if isinstance(match_data, dict):
                                for skill_info in match_data.values():
                                    if isinstance(skill_info, dict) and "doc_node_value" in skill_info:
                                        skill_name = skill_info["doc_node_value"].strip().lower()
                                        if skill_name:
                                            skills.add(skill_name)
                            # Si es una lista (nueva versi√≥n)
                            elif isinstance(match_data, list):
                                for skill_info in match_data:
                                    if isinstance(skill_info, dict) and "doc_node_value" in skill_info:
                                        skill_name = skill_info["doc_node_value"].strip().lower()
                                        if skill_name:
                                            skills.add(skill_name)
            
            logger.info(f"üîç skillNer encontr√≥: {skills}")
        except Exception as e:
            logger.error(f"‚ùå Error en _extract_with_skillner: {e}", exc_info=True)
        
        return skills

    def _extract_with_spacy_matcher(self, text: str) -> Set[str]:
        """Extrae habilidades usando patrones personalizados con Matcher de spaCy."""
        skills = set()
        
        if not self._nlp or not self._matcher:
            return skills
        
        try:
            doc = self._nlp(text)
            matches = self._matcher(doc)
            
            for match_id, start, end in matches:
                # Obtener el nombre de la categor√≠a (programaci√≥n, framework, etc.)
                category = self._nlp.vocab.strings[match_id]
                # Obtener el texto coincidente
                span = doc[start:end]
                skill_name = span.text.lower()
                skills.add(skill_name)
            
            logger.info(f"üîç spaCy Matcher encontr√≥: {skills}")
        except Exception as e:
            logger.error(f"‚ùå Error en _extract_with_spacy_matcher: {e}")
        
        return skills

    def _extract_with_ngrams(self, text: str) -> Set[str]:
        """Detecta posibles habilidades utilizando an√°lisis de n-gramas."""
        skills = set()
        
        try:
            # Normalizar texto
            text_lower = text.lower()
            text_normalized = unidecode.unidecode(text_lower)
            
            # Tokenizar y generar n-gramas (1, 2 y 3 palabras)
            tokens = word_tokenize(text_normalized)
            candidates = []
            
            # Unigrams
            candidates.extend(tokens)
            
            # Bigrams
            if len(tokens) >= 2:
                bigrams_list = list(ngrams(tokens, 2))
                bigrams_text = [' '.join(bg) for bg in bigrams_list]
                candidates.extend(bigrams_text)
            
            # Trigrams
            if len(tokens) >= 3:
                trigrams_list = list(ngrams(tokens, 3))
                trigrams_text = [' '.join(tg) for tg in trigrams_list]
                candidates.extend(trigrams_text)
            
            # Verificar candidatos contra bases de habilidades
            for candidate in candidates:
                # 1. Verificar en el skill_db
                if candidate in self._skill_db:
                    skills.add(candidate)
                    continue
                    
                # 2. Verificar en t√©rminos comunes
                for category, terms in self._common_terms.items():
                    if candidate in terms:
                        skills.add(candidate)
                        break
            
            logger.info(f"üîç An√°lisis de n-gramas encontr√≥: {skills}")
        except Exception as e:
            logger.error(f"‚ùå Error en _extract_with_ngrams: {e}")
        
        return skills

    def _extract_with_transformers(self, text: str) -> Set[str]:
        """Extrae posibles habilidades usando un modelo de NER basado en transformers."""
        skills = set()
        
        if not self.ner_pipeline:
            return skills
        
        try:
            # Obtener entidades reconocidas
            ner_results = self.ner_pipeline(text)
            
            # Filtrar por categor√≠as relevantes (ORG, TECH, PRODUCT)
            potential_skills = []
            for item in ner_results:
                if item['entity'] in ['B-ORG', 'I-ORG', 'B-PRODUCT', 'I-PRODUCT']:
                    potential_skills.append(item['word'].lower().replace('##', ''))
            
            # Reconstruir palabras fragmentadas por tokenizaci√≥n
            i = 0
            current_skill = ""
            while i < len(potential_skills):
                if potential_skills[i].startswith('##'):
                    current_skill += potential_skills[i].replace('##', '')
                else:
                    if current_skill:
                        skills.add(current_skill)
                    current_skill = potential_skills[i]
                i += 1
            
            if current_skill:
                skills.add(current_skill)
            
            # Verificar contra diccionarios para evitar falsos positivos
            filtered_skills = set()
            for skill in skills:
                # Verificar en el skill_db o en t√©rminos comunes
                if skill in self._skill_db:
                    filtered_skills.add(skill)
                    continue
                
                for terms in self._common_terms.values():
                    if skill in terms:
                        filtered_skills.add(skill)
                        break
            
            logger.info(f"üîç NER con transformers encontr√≥: {filtered_skills}")
            return filtered_skills
        except Exception as e:
            logger.error(f"‚ùå Error en _extract_with_transformers: {e}")
        
        return skills

    def _extract_with_heuristics(self, text: str) -> Set[str]:
        """Aplica reglas heur√≠sticas espec√≠ficas para encontrar habilidades."""
        skills = set()
        
        try:
            text_lower = text.lower()
            text_normalized = unidecode.unidecode(text_lower)
            
            # Patrones para habilidades t√©cnicas espec√≠ficas
            tech_patterns = {
                r'\b(?:program|code|develop)\s+(?:in|with)?\s+([a-z\+\#]+)': 'programming_languages',
                r'\bexperien(?:ce|cia)\s+(?:with|en|con)?\s+([a-z\+\#\.]+)': 'general',
                r'\bconoc(?:er|imiento)\s+(?:de|en)?\s+([a-z\+\#\.]+)': 'general',
                r'\b(?:expert|experiencia|conocimientos)\s+(?:in|on|en|de)?\s+([a-z\+\#\.]+)': 'general',
                r'\bhabil(?:idad|idades)\s+(?:en|con|de)?\s+([a-z\+\#\.]+)': 'general',
                r'\b(?:using|utilizando|con)\s+([a-z\+\#\.]+)': 'tools',
                r'\b(?:database|bd|bbdd|base de datos)\s+([a-z\+\#\.]+)': 'databases',
                r'\b(?:framework|librer√≠a|biblioteca|library)\s+([a-z\+\#\.]+)': 'frameworks'
            }
            
            # Buscar coincidencias con los patrones
            for pattern, category in tech_patterns.items():
                matches = re.finditer(pattern, text_normalized)
                for match in matches:
                    potential_skill = match.group(1).strip()
                    
                    # Verificar si es una habilidad v√°lida (en diccionarios o t√©rminos comunes)
                    if potential_skill in self._skill_db:
                        skills.add(potential_skill)
                        continue
                    
                    # Verificar en t√©rminos comunes por categor√≠a
                    if category in self._common_terms and potential_skill in self._common_terms[category]:
                        skills.add(potential_skill)
                    elif category == 'general':
                        # Para la categor√≠a general, buscar en todas las categor√≠as
                        for terms in self._common_terms.values():
                            if potential_skill in terms:
                                skills.add(potential_skill)
                                break
            
            # Detectar versiones de lenguajes (Python 3.9, Java 11, etc.)
            version_pattern = r'\b(python|java|php|javascript|typescript|c\+\+|c\#)\s+(\d+(?:\.\d+)?)+'
            for match in re.finditer(version_pattern, text_normalized):
                lang = match.group(1)
                version = match.group(2)
                skills.add(f"{lang} {version}")
                skills.add(lang)  # Tambi√©n agregar el lenguaje sin versi√≥n
            
            # Buscar acr√≥nimos conocidos (NLP, ML, AI, etc.)
            acronyms = {
                "nlp": "natural language processing",
                "ml": "machine learning",
                "ai": "artificial intelligence",
                "dl": "deep learning",
                "cv": "computer vision",
                "api": "api development",
                "oop": "object oriented programming",
                "ci/cd": "continuous integration",
                "aws": "amazon web services",
                "gcp": "google cloud platform",
                "ui/ux": "user interface design"
            }
            
            # Buscar acr√≥nimos en el texto
            for acronym, full_term in acronyms.items():
                pattern = r'\b' + re.escape(acronym) + r'\b'
                if re.search(pattern, text_lower):
                    skills.add(acronym)
                    skills.add(full_term)
            
            logger.info(f"üîç An√°lisis heur√≠stico encontr√≥: {skills}")
        except Exception as e:
            logger.error(f"‚ùå Error en _extract_with_heuristics: {e}")
        
        return skills

    def fallback_skill_detection(self, text: str) -> Set[str]:
        """
        M√©todo fallback mejorado para cuando otros m√©todos fallan o no devuelven resultados.
        Utiliza las mismas categor√≠as cargadas en com√∫n para mantener coherencia.
        """
        skills = set()
        
        # Usar los t√©rminos ya cargados
        fallback_lists = self._common_terms
        
        # Buscar coincidencias en el texto
        text_lower = text.lower()
        for category, terms in fallback_lists.items():
            for term in terms:
                if term in text_lower:
                    skills.add(term)
        
        # Buscar versiones espec√≠ficas
        version_pattern = r'\b(python|java|javascript|react)\s+(\d+(?:\.\d+)?)\b'
        for match in re.finditer(version_pattern, text_lower):
            skills.add(f"{match.group(1)} {match.group(2)}")
        
        logger.info(f"üìå Fallback encontr√≥: {skills}")
        return skills
    
    def __del__(self):
        if hasattr(self, 'skill_cache') and isinstance(self.skill_cache, shelve.DbfilenameShelf):
            self.skill_cache.close()

# ‚úÖ Lazy Load NLPProcessor
class NLPProcessor:
    def __init__(self, language: str = "es"):
        self.language = language
        self._nlp = None
        self._matcher = None
        self._gpt_handler = None
        self._sentiment_analyzer = None
        self.gpt_cache = TTLCache(maxsize=1000, ttl=3600)

    @property
    def nlp(self):
        if self._nlp is None:
            self._nlp = load_nlp_model(self.language)
        return self._nlp

    @property
    def matcher(self):
        if self._matcher is None:
            self._matcher = Matcher(self.nlp.vocab)
        return self._matcher

    @property
    def sentiment_analyzer(self):
        if self._sentiment_analyzer is None:
            self._sentiment_analyzer = RoBertASentimentAnalyzer()
        return self._sentiment_analyzer

    async def initialize_gpt_handler(self):
        if self._gpt_handler is None:
            self._gpt_handler = await get_gpt_client()

    def set_language(self, language: str):
        self.language = language
        self.nlp = load_nlp_model(language)
        self.matcher = Matcher(self.nlp.vocab) if self.nlp else None
        logger.info(f"üîÑ Modelo NLP cambiado a: {language}")

    def define_intent_patterns(self) -> None:
        pass  # Ejemplo sin cambios

    async def analyze(self, text: str) -> dict:
        if not self.nlp:
            logger.error("No se ha cargado el modelo spaCy, devolviendo an√°lisis vac√≠o.")
            return {"intents": [], "entities": [], "sentiment": "neutral", "detected_divisions": []}
        from app.chatbot.utils import clean_text
        cleaned_text = clean_text(text)
        doc = await asyncio.to_thread(self.nlp, cleaned_text)
        entities = [(ent.text, ent.label_) for ent in doc.ents]
        matches = await asyncio.to_thread(self.matcher, doc)
        intents = [self.nlp.vocab.strings[m[0]] for m in matches]
        sentiment = await asyncio.to_thread(self.sentiment_analyzer.analyze_sentiment, cleaned_text)
        # ...
        return {
            "entities": entities,
            "intents": intents,
            "sentiment": sentiment,
            "detected_divisions": []
        }

    @cachedmethod(lambda self: self.gpt_cache)
    async def extract_skills(self, text: str, business_unit: str = "huntRED¬Æ") -> Dict[str, List[str]]:
        extractor = SkillExtractorManager.get_instance(self.language)
        if not extractor:
            logger.warning("‚ö†Ô∏è SkillExtractor no est√° inicializado")
            return {"skills": []}
        try:
            from app.chatbot.utils import clean_text
            cleaned_text = clean_text(text)
            skills_result = extractor.extract_skills(cleaned_text)
            logger.info(f"üìä Habilidades extra√≠das: {skills_result['skills']}")
            return skills_result
        except Exception as e:
            logger.error(f"‚ùå Error extrayendo habilidades: {e}", exc_info=True)
            return {"skills": []}

    def extract_interests_and_skills(self, text: str) -> dict:
        text_normalized = unidecode.unidecode(text.lower())
        skills = set()
        priorities = {}
        all_skills = get_all_skills_for_unit()

        for skill in all_skills:
            skill_normalized = unidecode.unidecode(skill.lower())
            if re.search(r'\b' + re.escape(skill_normalized) + r'\b', text_normalized):
                skills.add(skill)
                priorities[skill] = 2

        if self._nlp:
            doc = self.nlp(text)
            # ...
        prioritized_interests = prioritize_interests(list(skills))
        return {"skills": list(skills), "prioritized_skills": prioritized_interests}

    def infer_gender(self, name: str) -> str:
        GENDER_DICT = {"jose": "M", "maria": "F", "andrea": "F", "juan": "M"}
        parts = name.lower().split()
        m_count = sum(1 for p in parts if p in GENDER_DICT and GENDER_DICT[p] == "M")
        f_count = sum(1 for p in parts if p in GENDER_DICT and GENDER_DICT[p] == "F")
        return "M" if m_count > f_count else "F" if f_count > m_count else "O"

    def extract_skills_and_roles(self, text: str, business_unit: str = "huntRED¬Æ") -> dict:
        text_normalized = unidecode.unidecode(text.lower())
        skills = set()
        priorities = {}
        try:
            all_skills = get_all_skills_for_unit(business_unit)
        except Exception as e:
            logger.error(f"Error obteniendo habilidades para {business_unit}: {e}")
            all_skills = []

        for skill in all_skills:
            skill_normalized = unidecode.unidecode(skill.lower())
            if re.search(r'\b' + re.escape(skill_normalized) + r'\b', text_normalized):
                skills.add(skill)
                priorities[skill] = 2

        extractor = SkillExtractorManager.get_instance(self.language)
        if extractor:
            try:
                skill_results = extractor.extract_skills(text)
                extracted_skills = set(skill_results.get("skills", []))
                skills.update(extracted_skills)
                for skill in extracted_skills:
                    priorities[skill] = priorities.get(skill, 1)
                logger.info(f"üß† Habilidades extra√≠das por SkillExtractorManager: {extracted_skills}")
            except Exception as e:
                logger.error(f"‚ùå Error en SkillExtractorManager: {e}", exc_info=True)

        skills_list = list(skills)
        weighted_skills = prioritize_interests(skills_list) if skills_list else {}
        suggested_roles = get_positions_by_skills(skills, weighted_skills) if skills else []
        return {
            "skills": skills_list,
            "suggested_roles": suggested_roles
        }

# ============== GPT CLIENT POOL / Tabiya y Robert  ==============
# ‚úÖ Pool de clientes para GPTHandler (Lazy Load)
GPT_CLIENT_POOL_SIZE = 5
gpt_client_pool = [None] * GPT_CLIENT_POOL_SIZE
gpt_client_locks = [asyncio.Lock() for _ in range(GPT_CLIENT_POOL_SIZE)]

async def get_gpt_client():
    """Devuelve un cliente GPT disponible del pool."""
    for i in range(GPT_CLIENT_POOL_SIZE):
        async with gpt_client_locks[i]:
            if gpt_client_pool[i] is None:
                gpt_client_pool[i] = GPTHandler()
                await gpt_client_pool[i].initialize()
            return gpt_client_pool[i]
    return GPTHandler()  # Si todos est√°n ocupados, crea uno nuevo.


class TabiyaJobClassifier:
    def __init__(self):
        from tabiya_livelihoods_classifier.inference.linker import EntityLinker
        self.linker = EntityLinker()

    def classify(self, text):
        return self.linker.link_text(text)

class RoBertASentimentAnalyzer:
    def __init__(self, model_name="cardiffnlp/twitter-roberta-base-sentiment-latest"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)

    def analyze_sentiment(self, text):
        """Analiza el sentimiento del texto."""
        inputs = self.tokenizer(text, return_tensors="pt")
        with torch.no_grad():
            outputs = self.model(**inputs)
            predicted_class = torch.argmax(outputs.logits, dim=1).item()
        return ["negative", "neutral", "positive"][predicted_class]
    
# AQU√ç agregamos la funci√≥n:
def get_skill_extractor(language: str = "es"):
    """
    Devuelve una instancia √∫nica de SkillExtractorManager
    para el idioma especificado (por defecto 'es').p
    """
    return SkillExtractorManager.get_instance(language)
# Instancia global del procesador
nlp_processor = NLPProcessor(language="es")

# ============== EJEMPLO MAIN ==============
if __name__ == "__main__":
    # 1) (Opcional) Parse RDF local de ESCO
    parse_esco_rdf_to_json("esco.ttl", "esco_from_rdf.json")
    
    # 2) Extraer data desde las CLASES DE EXTRACTORS (APIs, XLS, CSV, etc.)
    # Ejemplo: instanciamos cada una seg√∫n necesitemos
    esco_ext = ESCOExtractor()
    onet_ext = ONETExtractor()
    nice_ext = NICEExtractor()
    career_ext = CareerOneStopExtractor()

    # Recogemos datos de ESCO (API) en espa√±ol
    esco_skills_data = esco_ext.get_skills(language="es", limit=5)
    esco_occ_data = esco_ext.get_occupations(language="es", limit=5)

    # O from NICE XLSX
    nice_data = nice_ext.get_skills(sheet_name="Skills")
    
    # ... y as√≠ con O*NET o CareerOneStop
    onet_data = onet_ext.get_occupations(api_key="...", count=5)
    career_data = career_ext.get_careers(api_user_id="...", api_key="...", keyword="cybersecurity", limit=5)

    # 3) Unificamos los datos en un solo formato (unify_data)
    #    Por ahora, ejemplo con ESCO + NICE
    unified = unify_data(esco_skills_data, esco_occ_data, nice_data)
    logger.info(f"Datos unificados: {len(unified)}")

    # 4) (Opcional) Guardar estos datos en un .json (similar a "esco_skills.json"), 
    #    o prepara un "merger" para combinarlos con otras fuentes
    #    -> En caso de que quieras que generen un archivo tipo "ESCO_API_skills.json", etc.

    # EJEMPLO r√°pido: guardamos un 'esco_api_skills.json' con lo que unificamos
    with open(os.path.join(CONFIG["OUTPUT_DIR"], "esco_api_skills.json"), "w", encoding="utf-8") as out:
        json.dump({f"gen_{i}": item for i, item in enumerate(unified)}, out, indent=2, ensure_ascii=False)

    # 5) Fusionamos con el resto de bases para tener un combined_skills.json
    merger = SkillDBMerger(CONFIG["OUTPUT_DIR"], "combined_skills.json")
    merger.merge()

    # 6) Probar pipeline b√°sico
    pipeline = SkillExtractionPipeline(combined_db_path=CONFIG["COMBINED_DB_PATH"])
    texts = [
        "Busco un desarrollador con experiencia en Python, Django y machine learning.",
        "Me interesa contratar a alguien con habilidades en Java, React y gesti√≥n de proyectos."
    ]
    for txt in texts:
        print(f"[Pipeline B√°sico] {txt} -> {pipeline.extract_skills(txt)}")

    # 7) Probar extractor avanzado
    advanced_extractor = SkillExtractorManager.get_instance("es")
    for txt in texts:
        print(f"[Avanzado] {txt} -> {advanced_extractor.extract_skills(txt)['skills']}")

    # 8) Probar NLPProcessor
    nlp_proc = NLPProcessor(language="es")
    async def test_nlp():
        for txt in texts:
            res = await nlp_proc.extract_skills(txt)
            print(f"[NLPProcessor] {txt} -> {res['skills']}")
    asyncio.run(test_nlp())





    _______________
    # /home/pablollh/app/chatbot/nlp.py
import time
import psutil
import os
import sys
import subprocess
import spacy
import json
import logging
from typing import Dict, List, Optional, Set, Union, Literal
from cachetools import TTLCache
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from langdetect import detect
from threading import Lock
from datetime import datetime

# Importaciones condicionales
try:
    from deep_translator import GoogleTranslator
    TRANSLATOR_AVAILABLE = True
except ImportError:
    TRANSLATOR_AVAILABLE = False

try:
    from transformers import pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

logger = logging.getLogger(__name__)
log_file = '/home/pablollh/logs/nlp.log'
try:
    handler = logging.FileHandler(log_file)
    handler.setFormatter(logging.Formatter('%(asctime)s %(levelname)s %(name)s %(message)s'))
    logger.addHandler(handler)
except PermissionError:
    logger.addHandler(logging.StreamHandler(sys.stdout))
    logger.warning(f"No se pudo escribir en {log_file}. Usando stdout.")

# Modelos spaCy
MODEL_LANGUAGES = {
    "es": {"sm": "es_core_news_sm", "md": "es_core_news_md", "lg": "es_core_news_lg"},
    "en": {"sm": "en_core_web_sm", "md": "en_core_web_md", "lg": "en_core_web_lg"},
    "fr": {"sm": "fr_core_news_sm", "md": "fr_core_news_md", "lg": "fr_core_news_lg"},
    "default": {"sm": "xx_ent_wiki_sm", "md": "xx_ent_wiki_sm", "lg": "xx_ent_wiki_sm"}
}

# Directorios de cat√°logos
CATALOG_BASE_PATH = "/home/pablollh/skills_data"
UTILITIES_CATALOG_PATH = "/home/pablollh/app/utilidades/catalogs"

CATALOGS = {
    "global": {"esco": os.path.join(CATALOG_BASE_PATH, "ESCO_occup_skills.json")},
    "es": {
        "occupations": os.path.join(CATALOG_BASE_PATH, "occupations_es.json"),
        "skills": os.path.join(CATALOG_BASE_PATH, "skills_es.json"),
        "skills_relax": os.path.join(CATALOG_BASE_PATH, "skill_db_relax_20.json"),
        "skills_opportunities": os.path.join(UTILITIES_CATALOG_PATH, "skills.json")
    }
}

# Configuraci√≥n global
MAX_SKILLS = 20000  # Aumentado para cubrir m√°s casos, ajustable seg√∫n RAM
USE_TABIYA = False  # Cambia a True si integras Tabiya
json_lock = Lock()
nltk.download('vader_lexicon', quiet=True)
SIA = SentimentIntensityAnalyzer()

class CatalogManager:
    def __init__(self, catalogs: Dict = CATALOGS):
        self.catalogs = catalogs
        self.cached_data = TTLCache(maxsize=10, ttl=21600)  # 6 horas de cach√©

    def get_catalog_path(self, category: str, catalog_name: str) -> str:
        if category in self.catalogs and catalog_name in self.catalogs[category]:
            return self.catalogs[category][catalog_name]
        raise ValueError(f"Cat√°logo {catalog_name} no encontrado en categor√≠a {category}")

    def load_catalog(self, category: str, catalog_name: str) -> Union[Dict, List]:
        path = self.get_catalog_path(category, catalog_name)
        cache_key = f"{category}_{catalog_name}"
        if cache_key in self.cached_data:
            return self.cached_data[cache_key]
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            self.cached_data[cache_key] = data
            logger.info(f"Cat√°logo cargado: {path}")
            return data
        except Exception as e:
            logger.error(f"Error cargando cat√°logo {category}/{catalog_name}: {e}")
            return {}

    def get_all_skills(self, categories_and_catalogs: List[tuple], max_skills: int = MAX_SKILLS) -> Set[str]:
        skills = set()
        
        def extract_skills(data, skills_set: Set[str]) -> bool:
            if len(skills_set) >= max_skills:
                return True
            if isinstance(data, dict):
                for key, value in data.items():
                    if key in ["Habilidades T√©cnicas", "Habilidades Blandas", "Certificaciones", "Herramientas"]:
                        if isinstance(value, list):
                            skills_set.update(s.lower() for s in value if isinstance(s, str))
                    elif extract_skills(value, skills_set):
                        return True
            elif isinstance(data, list):
                for item in data:
                    if extract_skills(item, skills_set):
                        return True
            elif isinstance(data, str):
                skills_set.add(data.lower())
            if len(skills_set) >= max_skills:
                logger.warning(f"L√≠mite de {max_skills} habilidades alcanzado")
                return True
            return False

        for category, catalog_name in categories_and_catalogs:
            catalog = self.load_catalog(category, catalog_name)
            if not catalog:
                continue
            if extract_skills(catalog, skills):
                break

        logger.info(f"Total habilidades cargadas: {len(skills)}")
        return skills

class ModelManager:
    def __init__(self):
        self.spacy_cache = TTLCache(maxsize=5, ttl=21600)
        self.transformer_cache = TTLCache(maxsize=3, ttl=21600)

    def get_spacy_model(self, lang: str, size: str = 'sm') -> spacy.language.Language:
        key = f"{lang}_{size}"
        if key in self.spacy_cache:
            return self.spacy_cache[key]
        model_name = MODEL_LANGUAGES.get(lang, MODEL_LANGUAGES["default"]).get(size, "xx_ent_wiki_sm")
        try:
            model = spacy.load(model_name)
        except OSError:
            logger.info(f"Descargando modelo {model_name}...")
            subprocess.check_call([sys.executable, "-m", "spacy", "download", model_name])
            model = spacy.load(model_name)
        self.spacy_cache[key] = model
        return model

    def get_transformer_pipeline(self, task: str, model_path: str) -> Optional[pipeline]:
        if not TRANSFORMERS_AVAILABLE:
            return None
        key = f"{task}_{model_path}"
        if key in self.transformer_cache:
            return self.transformer_cache[key]
        try:
            pipe = pipeline(task, model=model_path, device=-1)
            self.transformer_cache[key] = pipe
            logger.info(f"Transformer cargado: {model_path}")
            return pipe
        except Exception as e:
            logger.error(f"Error cargando transformer {model_path}: {e}")
            return None

class NLPProcessor:
    def __init__(self, language: str = 'es', mode: Literal['candidate', 'opportunity'] = 'opportunity', analysis_depth: Literal['quick', 'deep'] = 'quick'):
        self.language = language
        self.mode = mode
        self.analysis_depth = analysis_depth
        self.catalog_mgr = CatalogManager()
        self.model_mgr = ModelManager()
        self.nlp = self.model_mgr.get_spacy_model(language, 'sm' if analysis_depth == 'quick' else 'md')
        
        self.catalog_sources = [
            ("es", "skills"),
            ("es", "occupations"),
            ("es", "skills_relax"),
            ("es", "skills_opportunities"),
            ("global", "esco")
        ]
        
        self.skills = self.catalog_mgr.get_all_skills(self.catalog_sources, max_skills=MAX_SKILLS)
        self.phrase_matcher = self._build_phrase_matcher()
        self.sentiment_analyzer = SIA if analysis_depth == 'quick' or not TRANSFORMERS_AVAILABLE else self.model_mgr.get_transformer_pipeline('sentiment-analysis', 'cardiffnlp/twitter-roberta-base-sentiment-latest')
        self.translator = GoogleTranslator(source='auto', target='es') if TRANSLATOR_AVAILABLE and analysis_depth == 'deep' else None
        
        self._log_resources("Inicializaci√≥n completada")

    def _build_phrase_matcher(self) -> spacy.matcher.PhraseMatcher:
        matcher = spacy.matcher.PhraseMatcher(self.nlp.vocab, attr='LOWER')
        patterns = [self.nlp.make_doc(skill) for skill in self.skills]
        matcher.add("SKILL", patterns)
        return matcher

    def _log_resources(self, stage: str) -> None:
        process = psutil.Process(os.getpid())
        logger.info(f"{stage} - CPU: {process.cpu_percent(interval=None):.2f}%, Memoria: {process.memory_info().rss / 1024 / 1024:.2f} MB")

    def extract_skills(self, text: str) -> Dict[str, List[str]]:
        doc = self.nlp(text.lower())
        skills = {"technical": [], "soft": [], "certifications": [], "tools": []}
        matches = self.phrase_matcher(doc)
        for _, start, end in matches:
            skill = doc[start:end].text
            self._classify_skill(skill, skills)
        
        if self.analysis_depth == 'deep' and self.translator and detect(text) != 'es':
            try:
                translated = self.translator.translate(text)
                trans_doc = self.nlp(translated.lower())
                trans_matches = self.phrase_matcher(trans_doc)
                for _, start, end in trans_matches:
                    skill = trans_doc[start:end].text
                    self._classify_skill(skill, skills)
            except Exception as e:
                logger.error(f"Error en traducci√≥n: {e}")
        
        return {k: list(set(v)) for k, v in skills.items()}

    def _classify_skill(self, skill: str, skills_dict: Dict[str, List[str]]) -> None:
        skill_lower = skill.lower()
        for category, catalog_name in self.catalog_sources:
            catalog = self.catalog_mgr.load_catalog(category, catalog_name)
            if not catalog:
                continue
            found = False
            
            def check_skills(data, target: str) -> Optional[str]:
                if isinstance(data, dict):
                    for key, value in data.items():
                        if key in ["Habilidades T√©cnicas", "Habilidades Blandas", "Certificaciones", "Herramientas"]:
                            if isinstance(value, list) and target in [s.lower() for s in value if isinstance(s, str)]:
                                return key
                        elif result := check_skills(value, target):
                            return result
                elif isinstance(data, list):
                    if target in [s.lower() for s in data if isinstance(s, str)]:
                        return "technical"  # Default si no hay categor√≠a expl√≠cita
                elif isinstance(data, str) and target == data.lower():
                    return "technical"
                return None

            category_key = check_skills(catalog, skill_lower)
            if category_key:
                key_map = {
                    "Habilidades T√©cnicas": "technical",
                    "Habilidades Blandas": "soft",
                    "Certificaciones": "certifications",
                    "Herramientas": "tools",
                    "technical": "technical"
                }
                skills_dict[key_map[category_key]].append(skill_lower)
                found = True
                break
            if found:
                break

    def analyze(self, text: str) -> Dict[str, any]:
        start_time = time.time()
        self._log_resources("Inicio an√°lisis")
        skills = self.extract_skills(text)
        sentiment = self.get_sentiment(text)
        result = {"skills": skills, "sentiment": sentiment["label"], "sentiment_score": sentiment["score"]}
        exec_time = time.time() - start_time
        self._log_resources(f"Fin an√°lisis (Tiempo: {exec_time:.2f}s)")
        return result

    def get_sentiment(self, text: str) -> Dict[str, Union[str, float]]:
        if not self.sentiment_analyzer:
            return {"label": "neutral", "score": 0.0}
        try:
            if self.analysis_depth == 'quick':
                scores = SIA.polarity_scores(text)
                compound = scores['compound']
                label = "positive" if compound >= 0.05 else "negative" if compound <= -0.05 else "neutral"
                return {"label": label, "score": abs(compound)}
            result = self.sentiment_analyzer(text[:512])[0]
            return {"label": result["label"], "score": result["score"]}
        except Exception as e:
            logger.error(f"Error en an√°lisis de sentimiento: {e}")
            return {"label": "neutral", "score": 0.0}

nlp_processor = NLPProcessor()

if __name__ == "__main__":
    sample_text = "Tengo experiencia en Python, liderazgo y gesti√≥n de proyectos."
    result = nlp_processor.analyze(sample_text)
    print(json.dumps(result, ensure_ascii=False, indent=2))



    ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________
    ________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

# /home/pablollh/app/chatbot/nlp.py
import os
import json
import time
import logging
import asyncio
import sys
import random
import weakref
import pandas as pd
import numpy as np
from functools import lru_cache
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text  # Necesario para registrar SentencepieceOp
import concurrent.futures
from langdetect import detect  # A√±adir esta l√≠nea al inicio del archivo
from django.core.cache import cache
from sklearn.metrics.pairwise import cosine_similarity
from geopy.distance import geodesic
from datetime import datetime
from tenacity import retry, stop_after_attempt, wait_exponential
from typing import Dict, List, Any, Optional
from app.chatbot.migration_check import skip_on_migrate

# Configuraci√≥n de logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    handlers=[logging.FileHandler('/home/pablollh/logs/nlp.log'), logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# Cargar el Universal Sentence Encoder multiling√ºe
embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder-multilingual/3")

# Constantes
CACHE_TIMEOUT = 600
TRANSLATION_RATE_LIMIT = 5
TRANSLATION_DAILY_LIMIT = 200000
MAX_BATCH_SIZE = 50
RETRY_ATTEMPTS = 3

# Rutas de archivos
FILE_PATHS = {
    "relax_skills": "/home/pablollh/skills_data/skill_db_relax_20.json",
    "esco_skills": "/home/pablollh/skills_data/ESCO_occup_skills.json",
    "tabiya_skills": "/home/pablollh/skills_data/tabiya/tabiya-esco-v1.1.1/csv/skills.csv",
    "opportunity_catalog": "/home/pablollh/app/utilidades/catalogs/skills.json",
    "intents": "/home/pablollh/chatbot_data/intents.json"
}

# Configuraciones
TRANSLATION_CONFIG = {'RATE_LIMIT': 5, 'DAILY_LIMIT': 200000, 'BATCH_SIZE': 50, 'TIMEOUT': 30, 'RETRY_ATTEMPTS': 3, 'BACKOFF_FACTOR': 2}
CACHE_CONFIG = {'EMBEDDINGS_TTL': 3600, 'CATALOG_TTL': 86400, 'TRANSLATION_TTL': 1800}
MODEL_CONFIG = {'MAX_SEQUENCE_LENGTH': 128, 'SIMILARITY_THRESHOLD': 0.8, 'CONFIDENCE_THRESHOLD': 0.7, 'BATCH_SIZE': 32}
# Cat√°logos globales
CANDIDATE_CATALOG = None
OPPORTUNITY_CATALOG = None
INTENTS_CATALOG = None

# Lazy-load solo para configuraciones pesadas
def initialize_nlp_dependencies():
    """Inicializa configuraciones pesadas solo cuando se necesitan"""
    if not hasattr(initialize_nlp_dependencies, 'initialized'):
        import nltk
        nltk.download('vader_lexicon', quiet=True)
        initialize_nlp_dependencies.initialized = True

class RateLimitedTranslator:
    def __init__(self, source='auto', target='en'):
        initialize_nlp_dependencies()  # Configuraciones pesadas
        from deep_translator import GoogleTranslator
        from concurrent.futures import ThreadPoolExecutor
        from asyncio import Semaphore
        self.translator = GoogleTranslator(source=source, target=target)
        self.request_semaphore = Semaphore(TRANSLATION_CONFIG['RATE_LIMIT'])
        self.daily_request_count = 0
        self.last_reset_time = time.time()
        self.executor = ThreadPoolExecutor(max_workers=5)
        self._lock = asyncio.Lock()
        self._error_count = 0
        self._last_error_time = 0
        self._backoff_time = 1
        self._translation_cache = {}  # Cach√© local para traducciones

    async def _handle_error(self, e: Exception):
        """Manejo inteligente de errores con backoff exponencial"""
        async with self._lock:
            current_time = time.time()
            if current_time - self._last_error_time > 60:
                self._error_count = 0
                self._backoff_time = 1
            
            self._error_count += 1
            self._last_error_time = current_time
            
            if self._error_count > RETRY_ATTEMPTS:
                self._backoff_time = min(self._backoff_time * 2, 60)
                await asyncio.sleep(self._backoff_time)
                self._error_count = 0
            
            logger.error(f"Error en traducci√≥n: {e}", exc_info=True)

    async def _check_and_reset_daily_limit(self):
        async with self._lock:
            current_time = time.time()
            if current_time - self.last_reset_time >= 86400:  # 24 horas
                self.daily_request_count = 0
                self.last_reset_time = current_time

    @retry(stop=stop_after_attempt(RETRY_ATTEMPTS), wait=wait_exponential(multiplier=1, min=4, max=10))
    async def translate(self, text: str) -> str:
        if not text or len(text.strip()) < 1:
            logger.debug(f"Texto vac√≠o o inv√°lido para traducci√≥n: '{text}'")
            return text
        # Verificar cach√© primero
        cache_key = f"{text}_{self.translator.source}_{self.translator.target}"
        if cache_key in self._translation_cache:
            return self._translation_cache[cache_key]

        await self._check_and_reset_daily_limit()
        if self.daily_request_count >= TRANSLATION_CONFIG['DAILY_LIMIT']:
            logger.warning("L√≠mite diario de traducciones alcanzado")
            return text

        async with self.request_semaphore:
            loop = asyncio.get_event_loop()
            try:
                logger.debug(f"Traduciendo texto: '{text}'")
                translated = await loop.run_in_executor(
                    self.executor,
                    lambda: self.translator.translate(text)
                )
                async with self._lock:
                    self.daily_request_count += 1
                logger.debug(f"Traducci√≥n completada: '{translated}'")
                return translated
            except Exception as e:
                await self._handle_error(e)
                return text

    @retry(stop=stop_after_attempt(RETRY_ATTEMPTS), wait=wait_exponential(multiplier=1, min=4, max=10))
    async def translate_batch(self, texts: List[str]) -> List[str]:
        if not texts:
            logger.debug("Lista de textos vac√≠a para traducci√≥n")
            return []

        await self._check_and_reset_daily_limit()
        remaining_slots = TRANSLATION_CONFIG['DAILY_LIMIT'] - self.daily_request_count
        if remaining_slots <= 0:
            logger.warning("L√≠mite diario de traducciones alcanzado")
            return texts

        effective_batch = texts[:min(TRANSLATION_CONFIG['BATCH_SIZE'], remaining_slots)]
        async with self.request_semaphore:
            loop = asyncio.get_event_loop()
            try:
                logger.debug(f"Traduciendo lote de {len(effective_batch)} textos")
                translated = await loop.run_in_executor(
                    self.executor,
                    lambda: self.translator.translate_batch(effective_batch)
                )
                async with self._lock:
                    self.daily_request_count += len(effective_batch)
                logger.debug(f"Traducci√≥n de lote completada: {len(translated)} textos")
                return translated + texts[len(effective_batch):]
            except Exception as e:
                await self._handle_error(e)
                return texts

class AsyncTranslator:
    def __init__(self, source='auto', target='en'):
        self.rate_limited_translator = RateLimitedTranslator(source, target)

    async def translate(self, text: str) -> str:
        return await self.rate_limited_translator.translate(text)

    async def translate_batch(self, texts: List[str]) -> List[str]:
        return await self.rate_limited_translator.translate_batch(texts)

class CacheManager:
    def __init__(self):
        self.redis_client = cache
        self._local_cache = {}
        self._last_cleanup = time.time()
        self._lock = asyncio.Lock()

    async def get_or_set(self, key: str, getter_func, ttl: int = 3600):
        async with self._lock:
            if key in self._local_cache:
                value, expiry = self._local_cache[key]
                if time.time() < expiry:
                    return value

            value = self.redis_client.get(key)
            if value is not None:
                self._local_cache[key] = (value, time.time() + ttl)
                return value

            try:
                value = await getter_func()
                self.redis_client.set(key, value, timeout=ttl)
                self._local_cache[key] = (value, time.time() + ttl)
                return value
            except Exception as e:
                logger.error(f"Error al obtener o establecer en cach√©: {e}", exc_info=True)
                return None

    async def cleanup(self):
        async with self._lock:
            current_time = time.time()
            if current_time - self._last_cleanup > 3600:  # Limpiar cada hora
                self._local_cache = {k: v for k, v in self._local_cache.items() if current_time < v[1]}
                self._last_cleanup = current_time

class ModelPool:
    def __init__(self, max_models=3):
        self.max_models = max_models
        self.models = {}
        self.last_used = {}
        self._lock = asyncio.Lock()
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=1)

    async def get_model(self, model_type: str):
        async with self._lock:
            if model_type not in self.models:
                if len(self.models) >= self.max_models:
                    oldest = min(self.last_used.items(), key=lambda x: x[1])[0]
                    del self.models[oldest]
                    del self.last_used[oldest]
                
                try:
                    loop = asyncio.get_event_loop()
                    if model_type == "ner":
                        # Cargar modelo NER de TensorFlow Hub
                        self.models[model_type] = await loop.run_in_executor(
                            self.executor, lambda: hub.load("https://tfhub.dev/google/bert_multi_cased_L-12_H-768_A-12/4")
                        )
                    elif model_type == "intent":
                        # Cargar clasificador personalizado con tf.keras
                        self.models[model_type] = await loop.run_in_executor(
                            self.executor, self._load_intent_model
                        )
                    # No necesitamos "encoder" porque USE ya lo reemplaza
                    logger.info(f"Modelo {model_type} cargado")
                except Exception as e:
                    logger.error(f"Error al cargar modelo {model_type}: {e}", exc_info=True)
                    return None

            self.last_used[model_type] = time.time()
            return self.models[model_type]

    def _load_intent_model(self):
        # Modelo ligero de clasificaci√≥n con tf.keras
        model = tf.keras.Sequential([
            tf.keras.layers.Input(shape=(512,)),  # USE embeddings tienen 512 dimensiones
            tf.keras.layers.Dense(128, activation='relu'),
            tf.keras.layers.Dropout(0.2),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dense(len(self._load_intents()), activation='softmax')  # N√∫mero de intenciones
        ])
        model.load_weights("path/to/intent_model_weights.h5")  # Cargar pesos preentrenados
        return model

class BatchProcessor:
    def __init__(self, model_pool: ModelPool):
        self.model_pool = model_pool
        self.batch_size = MODEL_CONFIG['BATCH_SIZE']

    async def process_quick(self, preprocessed: Dict, catalog: Dict) -> Dict:
        skills = {"technical": [], "soft": [], "tools": [], "certifications": []}
        # Usar embeddings de USE para coincidencias r√°pidas
        text_emb = self._get_text_embedding(preprocessed["translated"])
        for category, skill_list in catalog.items():
            for skill_dict in skill_list:
                skill_emb = self._get_text_embedding(skill_dict["translated"])
                similarity = cosine_similarity([text_emb], [skill_emb])[0][0]
                if similarity > MODEL_CONFIG['SIMILARITY_THRESHOLD']:
                    skills[category].append(skill_dict)
        return skills

    async def process_deep(self, preprocessed: Dict, catalog: Dict) -> Dict:
        skills = {"technical": [], "soft": [], "tools": [], "certifications": []}
        ner_model = await self.model_pool.get_model("ner")
        
        if ner_model:
            try:
                # Procesar texto con modelo NER de TensorFlow Hub
                text_emb = self._get_text_embedding(preprocessed["translated"])
                # Aqu√≠ podr√≠as integrar un pipeline NER personalizado con tf.keras si es necesario
                # Por simplicidad, usamos coincidencias de embeddings
                for category, skill_list in catalog.items():
                    for skill_dict in skill_list:
                        skill_emb = self._get_text_embedding(skill_dict["translated"])
                        similarity = cosine_similarity([text_emb], [skill_emb])[0][0]
                        if similarity > MODEL_CONFIG['SIMILARITY_THRESHOLD']:
                            skills[category].append(skill_dict)
            except Exception as e:
                logger.error(f"Error procesando NER: {e}", exc_info=True)

        return skills

    def _get_text_embedding(self, text: str) -> np.ndarray:
        return embed([text]).numpy()[0]

    def _classify_entity(self, entity_group: str) -> str:
        if entity_group in ["SKILL", "TECH"]:
            return "technical"
        if entity_group == "TOOL":
            return "tools"
        if entity_group == "CERT":
            return "certifications"
        return "soft"

    def _get_skill_embedding(self, skill: str, encoder, tokenizer) -> np.ndarray:
        try:
            inputs = tokenizer(skill, return_tensors="tf", padding=True, truncation=True, max_length=MODEL_CONFIG['MAX_SEQUENCE_LENGTH'])
            outputs = encoder(inputs["input_ids"], attention_mask=inputs["attention_mask"])
            return tf.reduce_mean(outputs.last_hidden_state, axis=1).numpy()[0]
        except Exception as e:
            logger.error(f"Error obteniendo embedding de habilidad: {e}", exc_info=True)
            return np.zeros(768)

class PerformanceMonitor:
    def __init__(self):
        self.metrics = {}
        self._lock = asyncio.Lock()

    async def record_metric(self, name: str, value: float):
        async with self._lock:
            if name not in self.metrics:
                self.metrics[name] = []
            self.metrics[name].append(value)
            if len(self.metrics[name]) > 1000:
                self.metrics[name] = self.metrics[name][-1000:]

    def get_average(self, name: str) -> float:
        return np.mean(self.metrics.get(name, [0]))

    def get_metrics_report(self) -> Dict:
        return {
            name: {
                "avg": np.mean(values),
                "min": np.min(values),
                "max": np.max(values),
                "count": len(values)
            }
            for name, values in self.metrics.items()
        }

class NLPProcessor:
    @skip_on_migrate
    def __init__(self, mode: str = "candidate", analysis_depth: str = "quick", language: str = "es"):
        if 'migrate' in sys.argv:
            self.mode = mode
            self.depth = analysis_depth
            self.language = language
            return
        self.semaphore = asyncio.Semaphore(5)  # Limitar a 5 tareas concurrentes
        self.mode = mode
        self.depth = analysis_depth
        self.language = language
        self.last_used = time.time()
        self._translator = AsyncTranslator()
        self.candidate_catalog = None
        self.opportunity_catalog = None
        self.intents = None
        self.cache_manager = CacheManager()
        self.performance_monitor = PerformanceMonitor()
        logger.info(f"NLPProcessor inicializado: modo={mode}, profundidad={analysis_depth}, idioma={language}")

        initialize_nlp_dependencies()  # Inicializar dependencias solo aqu√≠

        self.mode = mode
        self.depth = analysis_depth
        self.language = language
        self.last_used = time.time()

        self._translator = AsyncTranslator()
        self._sentiment_analyzer = None
        self.api_key = os.getenv("GROK_API_KEY", "default_key")

        self.CATALOG_FILES = {
            "relax_skills": {"path": FILE_PATHS["relax_skills"], "type": "json", "process": self._process_relax_skills},
            "esco_skills": {"path": FILE_PATHS["esco_skills"], "type": "json", "process": self._process_esco_skills},
            "tabiya_skills": {"path": FILE_PATHS["tabiya_skills"], "type": "csv", "process": self._process_csv_skills}
        }

        self.candidate_catalog = None
        self.opportunity_catalog = None
        self.intents = None

        self.cache_manager = CacheManager()
        self.model_pool = ModelPool()
        self.batch_processor = BatchProcessor(self.model_pool)
        self._cache_embeddings = {}
        self.performance_monitor = PerformanceMonitor()
        logger.info(f"NLPProcessor inicializado: modo={mode}, profundidad={analysis_depth}, idioma={language}")

    async def initialize_gpt(self):
        if not self.gpt_handler:
            self.gpt_handler = GPTHandler()
            await self.gpt_handler.initialize()

    async def analyze_with_gpt(self, text: str, prompt: str) -> str:
        await self.initialize_gpt()
        response = await self.gpt_handler.generate_response(prompt + text)
        return response

    async def _ensure_catalogs_loaded(self):
        if self.candidate_catalog is None:
            self.candidate_catalog = await self._load_catalog_async("candidate") or {"technical": [], "soft": [], "tools": [], "certifications": []}
        if self.opportunity_catalog is None:
            self.opportunity_catalog = await self._load_opportunity_catalog() or []
        if self.intents is None:
            self.intents = self._load_intents() or {"default": ["No se pudieron cargar las intenciones"]}

    def _check_and_free_models(self, timeout=600):
        if time.time() - self.last_used > timeout:
            self._sentiment_analyzer = None
            self.model_pool.models.clear()
            self.model_pool.last_used.clear()
            logger.info("Modelos liberados por inactividad")
        self.last_used = time.time()

    def _load_sentiment_analyzer(self):
        if self._sentiment_analyzer is None:
            try:
                self._sentiment_analyzer = SentimentIntensityAnalyzer()
                logger.info("Analizador de sentimientos cargado")
            except Exception as e:
                logger.error(f"Error al cargar analizador de sentimientos: {e}", exc_info=True)
        return self._sentiment_analyzer

    async def _load_catalog_async(self, catalog_type: str) -> Dict[str, List[Dict[str, str]]]:
        cache_key = f"catalog_{catalog_type}"
        return await self.cache_manager.get_or_set(
            cache_key,
            lambda: self._load_catalog_impl(catalog_type),
            CACHE_CONFIG['CATALOG_TTL']
        )

    async def _load_catalog_impl(self, catalog_type: str) -> Dict[str, List[Dict[str, str]]]:
        catalog = {"technical": [], "soft": [], "tools": [], "certifications": []}
        for file_key, file_info in self.CATALOG_FILES.items():
            path = file_info["path"]
            if path in [FILE_PATHS["opportunity_catalog"], FILE_PATHS["intents"]]:
                logger.debug(f"Omitiendo archivo excluido: {path}")
                continue
            if not os.path.exists(path):
                logger.warning(f"Archivo no encontrado: {path}. Omitiendo.")
                continue
            try:
                if file_info["type"] == "json":
                    with open(path, "r", encoding="utf-8") as f:
                        data = json.load(f)
                elif file_info["type"] == "csv":
                    data = pd.read_csv(path)
                else:
                    logger.error(f"Tipo de archivo no soportado para {path}: {file_info['type']}")
                    continue
                await file_info["process"](data, catalog)
                logger.info(f"Cat√°logo cargado exitosamente desde {path}")
            except Exception as e:
                logger.error(f"Error al cargar {path}: {e}", exc_info=True)
                continue
        return catalog

    async def _process_relax_skills(self, data: Dict, catalog: Dict) -> None:
        skill_names = [skill_info.get("skill_name") for skill_info in data.values() if skill_info.get("skill_name")]
        skill_types = [skill_info.get("skill_type") for skill_info in data.values() if skill_info.get("skill_name")]
        if not skill_names:
            logger.debug("No hay habilidades v√°lidas en skill_db_relax_20.json")
            return
        langs = [detect(skill) for skill in skill_names]
        to_translate = [skill for skill, lang in zip(skill_names, langs) if lang != "en"]
        translated_skills = await self._translator.translate_batch(to_translate) if to_translate else []
        translated_iter = iter(translated_skills)
        for skill_name, skill_type, lang in zip(skill_names, skill_types, langs):
            if not skill_type:
                logger.debug(f"Entrada inv√°lida en skill_db_relax_20.json: {skill_name}")
                continue
            translated = next(translated_iter) if lang != "en" else skill_name
            skill_dict = {"original": skill_name, "translated": translated.lower(), "lang": lang}
            self._classify_skill(skill_dict, catalog)

    async def _process_esco_skills(self, data: Dict, catalog: Dict) -> None:
        for occupation, info in data.items():
            description = info.get("description", "")
            if not description:
                logger.debug(f"No se encontr√≥ descripci√≥n para la ocupaci√≥n {occupation}")
                continue
            logger.debug(f"Omitiendo ocupaci√≥n {occupation}: no se procesan habilidades directas")

    async def _process_csv_skills(self, data: pd.DataFrame, catalog: Dict) -> None:
        skills = data["PREFERREDLABEL"].dropna().tolist()
        if not skills:
            logger.debug("No hay habilidades v√°lidas en skills.csv")
            return
        translated_skills = await self._translator.translate_batch(skills)
        for skill, translated in zip(skills, translated_skills):
            if isinstance(translated, str):
                self._classify_skill({"original": skill, "translated": translated.lower(), "lang": "en"}, catalog)
            else:
                logger.warning(f"Traducci√≥n fallida para {skill}: {translated}")
                self._classify_skill({"original": skill, "translated": skill.lower(), "lang": "en"}, catalog)

    async def _load_opportunity_catalog(self) -> List[Dict]:
        cache_key = "opportunity_catalog"
        return await self.cache_manager.get_or_set(
            cache_key,
            self._load_opportunity_catalog_impl,
            CACHE_CONFIG['CATALOG_TTL']
        )

    async def _load_opportunity_catalog_impl(self) -> List[Dict]:
        opp_path = FILE_PATHS["opportunity_catalog"]
        if not os.path.exists(opp_path):
            logger.error(f"Archivo no encontrado: {opp_path}. Usando cat√°logo vac√≠o.")
            return []
        try:
            with open(opp_path, "r", encoding="utf-8") as f:
                opp_data = json.load(f)
            opportunities = []
            if isinstance(opp_data, dict):
                for business_unit, roles in opp_data.items():
                    for role, skills in roles.items():
                        skill_dict = {
                            "technical": [{"original": s, "translated": s.lower(), "lang": "es"} for s in skills.get("Habilidades T√©cnicas", [])],
                            "soft": [{"original": s, "translated": s.lower(), "lang": "es"} for s in skills.get("Habilidades Blandas", [])],
                            "certifications": [{"original": s, "translated": s.lower(), "lang": "es"} for s in skills.get("Certificaciones", [])],
                            "tools": [{"original": s, "translated": s.lower(), "lang": "es"} for s in skills.get("Herramientas", [])]
                        }
                        opportunities.append({
                            "title": f"{role} en {business_unit}",
                            "required_skills": skill_dict
                        })
            elif isinstance(opp_data, list):
                opportunities = opp_data
            else:
                logger.error(f"Formato no soportado en {opp_path}. Usando cat√°logo vac√≠o.")
                return []
            for opp in opportunities:
                for category in opp["required_skills"]:
                    skills = opp["required_skills"][category]
                    if skills:
                        originals = [s["original"] for s in skills]
                        translated = await self._translator.translate_batch(originals)
                        for skill, trans in zip(skills, translated):
                            if isinstance(trans, str):
                                skill["translated"] = trans.lower()
                            else:
                                logger.warning(f"Traducci√≥n fallida para {skill['original']}: {trans}")
                                skill["translated"] = skill["original"].lower()
            logger.info(f"Cat√°logo de oportunidades cargado con {len(opportunities)} oportunidades")
            return opportunities
        except Exception as e:
            logger.error(f"Error al cargar {opp_path}: {e}", exc_info=True)
            return []

    def _load_intents(self) -> Dict[str, List[str]]:
        cache_key = "intents_catalog"
        cached_catalog = cache.get(cache_key)
        if cached_catalog is not None:
            logger.info("Cat√°logo de intents cargado desde cach√©")
            return cached_catalog
        try:
            from app.chatbot.intents_handler import INTENTS
            intents_data = {intent: data["patterns"] for intent, data in INTENTS.items()}
            logger.info("Intents cargados desde intents_handler.py")
            cache.set(cache_key, intents_data, timeout=CACHE_CONFIG['CATALOG_TTL'])
            return intents_data
        except Exception as e:
            logger.error(f"Error al cargar intents: {e}", exc_info=True)
            return {"default": ["No se pudieron cargar las intenciones"]}

    async def preprocess(self, text: str) -> Dict[str, str]:
        if not text or len(text.strip()) < 3:
            return {"original": text.lower(), "translated": text.lower(), "lang": "unknown"}
        try:
            lang = detect(text)
            translated = await self._translator.translate(text) if lang != "en" else text
            return {"original": text.lower(), "translated": translated.lower(), "lang": lang}
        except Exception as e:
            logger.error(f"Error en preprocess: {e}", exc_info=True)
            return {"original": text.lower(), "translated": text.lower(), "lang": "unknown"}

    async def preprocess_batch(self, texts: List[str]) -> List[Dict[str, str]]:
        try:
            langs = [detect(text) if text and len(text.strip()) >= 3 else "unknown" for text in texts]
            to_translate = [text for text, lang in zip(texts, langs) if lang != "en"]
            translated_texts = await self._translator.translate_batch(to_translate) if to_translate else []
            translated_iter = iter(translated_texts)
            return [
                {"original": text.lower(), "translated": next(translated_iter) if lang != "en" else text.lower(), "lang": lang}
                if text and len(text.strip()) >= 3 else {"original": text.lower(), "translated": text.lower(), "lang": "unknown"}
                for text, lang in zip(texts, langs)
            ]
        except Exception as e:
            logger.error(f"Error en preprocess_batch: {e}", exc_info=True)
            return [{"original": text.lower(), "translated": text.lower(), "lang": "unknown"} for text in texts]

    async def extract_skills(self, text: str) -> Dict[str, List[Dict[str, str]]]:
        cache_key = f"skills_{hash(text)}"
        await self._ensure_catalogs_loaded()
        return await self.cache_manager.get_or_set(
            cache_key,
            lambda: self._extract_skills_impl(text),
            CACHE_CONFIG['EMBEDDINGS_TTL']
        )
    
    async def _extract_skills_impl(self, text: str) -> Dict[str, List[Dict[str, str]]]:
        preprocessed = await self.preprocess(text)
        catalog = self.candidate_catalog if self.mode == "candidate" else self.opportunity_catalog
        skills = {"technical": [], "soft": [], "tools": [], "certifications": []}
        text_emb = self.get_text_embedding(preprocessed["translated"])
        for category, skill_list in catalog.items():
            for skill_dict in skill_list:
                skill_emb = self.get_skill_embedding(skill_dict["translated"])
                similarity = cosine_similarity([text_emb], [skill_emb])[0][0]
                if similarity > MODEL_CONFIG['SIMILARITY_THRESHOLD']:
                    skills[category].append(skill_dict)
        return {"skills": skills}

    def _deduplicate_skills(self, skills: Dict[str, List[Dict[str, str]]]) -> Dict[str, List[Dict[str, str]]]:
        for category in skills:
            unique_skills = []
            embeddings = [self.get_skill_embedding(s["translated"]) for s in skills[category]]
            for i, emb in enumerate(embeddings):
                is_unique = True
                for j in range(i):
                    if cosine_similarity([emb], [embeddings[j]])[0][0] > 0.9:
                        is_unique = False
                        break
                if is_unique:
                    unique_skills.append(skills[category][i])
            skills[category] = unique_skills
        return {"skills": skills}

    def _classify_skill(self, skill_dict: Dict[str, str], catalog: Dict) -> None:
        try:
            skill_lower = skill_dict["translated"]
            if "cert" in skill_lower or "certificaci√≥n" in skill_lower:
                catalog["certifications"].append(skill_dict)
            elif "tool" in skill_lower or "herramienta" in skill_lower:
                catalog["tools"].append(skill_dict)
            elif "soft" in skill_lower or "blanda" in skill_lower:
                catalog["soft"].append(skill_dict)
            else:
                catalog["technical"].append(skill_dict)
        except Exception as e:
            logger.error(f"Error clasificando habilidad: {e}", exc_info=True)

    def get_text_embedding(self, text: str) -> np.ndarray:
        cache_key = f"embedding_{text}"
        cached_embedding = cache.get(cache_key)
        if cached_embedding is not None:
            return cached_embedding
        try:
            embedding = embed([text]).numpy()[0]
            cache.set(cache_key, embedding, timeout=CACHE_CONFIG['EMBEDDINGS_TTL'])
            return embedding
        except Exception as e:
            logger.error(f"Error obteniendo embedding de texto: {e}", exc_info=True)
            return np.zeros(512)  # USE devuelve embeddings de 512 dimensiones

    def get_skill_embedding(self, skill: str) -> np.ndarray:
        return self.get_text_embedding(skill)

    def analyze_sentiment(self, text: str) -> Dict[str, float]:
        sentiment_analyzer = self._load_sentiment_analyzer()
        if sentiment_analyzer:
            try:
                sentiment = sentiment_analyzer.polarity_scores(text)
                return {
                    "compound": sentiment["compound"],
                    "label": "positive" if sentiment["compound"] > 0.05 else "negative" if sentiment["compound"] < -0.05 else "neutral"
                }
            except Exception as e:
                logger.error(f"Error analizando sentimiento: {e}", exc_info=True)
        return {"compound": 0.0, "label": "neutral"}

    async def classify_intent(self, text: str) -> Dict[str, float]:
        try:
            intent_classifier = asyncio.run(self.model_pool.get_model("intent"))
            if intent_classifier:
                preprocessed = asyncio.run(self.preprocess(text))
                intent_result = intent_classifier(preprocessed["translated"])[0]
                confidence = intent_result["score"]
                intent = intent_result["label"] if confidence > MODEL_CONFIG['CONFIDENCE_THRESHOLD'] else "unknown"
                logger.debug(f"Intenci√≥n clasificada: {intent} con confianza {confidence}")
                return {"intent": intent, "confidence": confidence}
        except Exception as e:
            logger.error(f"Error clasificando intenci√≥n: {e}", exc_info=True)
        return {"intent": "unknown", "confidence": 0.0}

    def match_opportunities(self, candidate_skills: Dict[str, List[Dict[str, str]]]) -> List[Dict]:
        if self.mode != "candidate":
            return []
        asyncio.run(self._ensure_catalogs_loaded())
        try:
            matches = []
            candidate_emb = np.mean([self.get_skill_embedding(s["translated"]) for s in sum(candidate_skills.values(), [])] or [np.zeros(512)], axis=0)
            for opp in self.opportunity_catalog:
                opp_skills = sum(opp["required_skills"].values(), [])
                opp_emb = np.mean([self.get_skill_embedding(s["translated"]) for s in opp_skills] or [np.zeros(512)], axis=0)
                score = cosine_similarity([candidate_emb], [opp_emb])[0][0]
                matches.append({"opportunity": opp["title"], "match_score": float(score)})
            return sorted(matches, key=lambda x: x["match_score"], reverse=True)[:5]
        except Exception as e:
            logger.error(f"Error emparejando oportunidades: {e}", exc_info=True)
            return []

    def get_suggested_skills(self, business_unit: str, category: str = "general") -> List[str]:
        asyncio.run(self._ensure_catalogs_loaded())  # Asegurarse de que los cat√°logos est√©n cargados
        if not self.opportunity_catalog:
            logger.warning("Cat√°logo de oportunidades vac√≠o. No se pueden sugerir habilidades.")
            return []
        try:
            relevant_opps = [opp for opp in self.opportunity_catalog if business_unit.lower() in opp["title"].lower()]
            if not relevant_opps:
                logger.warning(f"No se encontraron oportunidades para {business_unit}. Usando cat√°logo general.")
                relevant_opps = self.opportunity_catalog
            suggested = set()
            for opp in relevant_opps:
                if category == "general":
                    for cat_skills in opp["required_skills"].values():
                        for skill in cat_skills:
                            suggested.add(skill["original"])
                else:
                    for skill in opp["required_skills"].get(category, []):
                        suggested.add(skill["original"])
            return list(suggested)
        except Exception as e:
            logger.error(f"Error obteniendo habilidades sugeridas: {e}", exc_info=True)
            return []

    async def analyze(self, text: str, language: str = "es") -> Dict:
        start_time = time.time()
        try:
            preprocessed = await self.preprocess(text)
            skills = await self.extract_skills(text)
            sentiment = self.analyze_sentiment(preprocessed["translated"])
            intent = await self.classify_intent(text) if self.depth == "quick" else {"intent": "unknown", "confidence": 0.0}

            result = {
                "skills": skills["skills"],
                "sentiment": sentiment["label"],
                "sentiment_score": abs(sentiment["compound"]),
                "intent": intent["intent"],
                "intent_confidence": intent["confidence"],
                "metadata": {
                    "execution_time": time.time() - start_time,
                    "original_text": preprocessed["original"],
                    "translated_text": preprocessed["translated"],
                    "detected_language": language or preprocessed["lang"]
                }
            }

            if self.mode == "candidate":
                result["opportunities"] = self.match_opportunities(skills["skills"])
            elif self.mode == "opportunity":
                result["required_skills"] = skills["skills"]
            elif self.depth == "deep":
                summary = await self.analyze_with_gpt(text, "Resume el siguiente texto en una frase: ")
                result["summary"] = summary

            logger.info(f"An√°lisis completado para texto: {text[:50]}... en {result['metadata']['execution_time']:.2f}s")
            return result
        except Exception as e:
            logger.error(f"Error en an√°lisis: {e}", exc_info=True)
            return {
                "skills": {"technical": [], "soft": [], "tools": [], "certifications": []},
                "sentiment": "neutral",
                "sentiment_score": 0.0,
                "intent": "unknown",
                "intent_confidence": 0.0,
                "metadata": {
                    "execution_time": time.time() - start_time,
                    "original_text": text.lower(),
                    "translated_text": text.lower(),
                    "detected_language": "unknown"
                }
            }
        

if __name__ == "__main__":
    nlp = NLPProcessor(mode="candidate", analysis_depth="quick")
    examples = {
        "candidate_quick": "Tengo experiencia en Python y trabajo en equipo.",
        "candidate_deep": "Soy un desarrollador con certificaci√≥n AWS y habilidades en Java.",
        "opportunity_quick": "Buscamos un ingeniero con experiencia en SQL y comunicaci√≥n."
    }
    for mode, text in examples.items():
        print(f"\nAnalizando en modo '{mode}':")
        nlp.depth = "quick" if "quick" in mode else "deep"
        result = asyncio.run(nlp.analyze(text))
        print(json.dumps(result, indent=2, ensure_ascii=False)) 